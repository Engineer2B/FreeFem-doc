



<!DOCTYPE html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      <meta http-equiv="x-ua-compatible" content="ie=edge">
      
      
      
      
        <meta name="lang:clipboard.copy" content="Copy to clipboard">
      
        <meta name="lang:clipboard.copied" content="Copied to clipboard">
      
        <meta name="lang:search.language" content="en">
      
        <meta name="lang:search.pipeline.stopwords" content="True">
      
        <meta name="lang:search.pipeline.trimmer" content="True">
      
        <meta name="lang:search.result.none" content="No matching documents">
      
        <meta name="lang:search.result.one" content="1 matching document">
      
        <meta name="lang:search.result.other" content="# matching documents">
      
        <meta name="lang:search.tokenizer" content="[\s\-]+">
      
      <link rel="shortcut icon" href="../../images/favicon.png">
      <meta name="generator" content="mkdocs-1.0.4, mkdocs-material-3.0.4">
    
    
      
        <title>Parallelization - FreeFem++</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/application.451f80e5.css">
      
      
    
    
      <script src="../../assets/javascripts/modernizr.1aa3b519.js"></script>
    
    
      
        <link href="https://fonts.gstatic.com" rel="preconnect" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700|Roboto+Mono">
        <style>body,input{font-family:"Roboto","Helvetica Neue",Helvetica,Arial,sans-serif}code,kbd,pre{font-family:"Roboto Mono","Courier New",Courier,monospace}</style>
      
    
    <link rel="stylesheet" href="../../assets/fonts/material-icons.css">
    
      <link rel="stylesheet" href="../../stylesheets/extra.css">
    
    
  </head>
  
    <body dir="ltr">
  
    <svg class="md-svg">
      <defs>
        
        
          <svg xmlns="http://www.w3.org/2000/svg" width="416" height="448"
    viewBox="0 0 416 448" id="__github">
  <path fill="currentColor" d="M160 304q0 10-3.125 20.5t-10.75 19-18.125
        8.5-18.125-8.5-10.75-19-3.125-20.5 3.125-20.5 10.75-19 18.125-8.5
        18.125 8.5 10.75 19 3.125 20.5zM320 304q0 10-3.125 20.5t-10.75
        19-18.125 8.5-18.125-8.5-10.75-19-3.125-20.5 3.125-20.5 10.75-19
        18.125-8.5 18.125 8.5 10.75 19 3.125 20.5zM360
        304q0-30-17.25-51t-46.75-21q-10.25 0-48.75 5.25-17.75 2.75-39.25
        2.75t-39.25-2.75q-38-5.25-48.75-5.25-29.5 0-46.75 21t-17.25 51q0 22 8
        38.375t20.25 25.75 30.5 15 35 7.375 37.25 1.75h42q20.5 0
        37.25-1.75t35-7.375 30.5-15 20.25-25.75 8-38.375zM416 260q0 51.75-15.25
        82.75-9.5 19.25-26.375 33.25t-35.25 21.5-42.5 11.875-42.875 5.5-41.75
        1.125q-19.5 0-35.5-0.75t-36.875-3.125-38.125-7.5-34.25-12.875-30.25-20.25-21.5-28.75q-15.5-30.75-15.5-82.75
        0-59.25 34-99-6.75-20.5-6.75-42.5 0-29 12.75-54.5 27 0 47.5 9.875t47.25
        30.875q36.75-8.75 77.25-8.75 37 0 70 8 26.25-20.5
        46.75-30.25t47.25-9.75q12.75 25.5 12.75 54.5 0 21.75-6.75 42 34 40 34
        99.5z" />
</svg>
        
      </defs>
    </svg>
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" data-md-component="overlay" for="__drawer"></label>
    
      <a href="../../#mpi" tabindex="1" class="md-skip">
        Skip to content
      </a>
    
    
      <header class="md-header" data-md-component="header">
  <nav class="md-header-nav md-grid">
    <div class="md-flex">
      <div class="md-flex__cell md-flex__cell--shrink">
        <a href="../.." title="FreeFem++" class="md-header-nav__button md-logo">
          
            <img src="../../images/favicon.png" width="24" height="24">
          
        </a>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        <label class="md-icon md-icon--menu md-header-nav__button" for="__drawer"></label>
      </div>
      <div class="md-flex__cell md-flex__cell--stretch">
        <div class="md-flex__ellipsis md-header-nav__title" data-md-component="title">
          
            
              <span class="md-header-nav__topic">
                FreeFem++
              </span>
              <span class="md-header-nav__topic">
                Parallelization
              </span>
            
          
        </div>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        
          
            <label class="md-icon md-icon--search md-header-nav__button" for="__search"></label>
            
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="query" data-md-state="active">
      <label class="md-icon md-search__icon" for="__search"></label>
      <button type="reset" class="md-icon md-search__icon" data-md-component="reset" tabindex="-1">
        &#xE5CD;
      </button>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="result">
          <div class="md-search-result__meta">
            Type to start searching
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
          
        
      </div>
      
        <div class="md-flex__cell md-flex__cell--shrink">
          <div class="md-header-nav__source">
            


  


  <a href="https://github.com/FreeFem/FreeFem-doc/" title="Go to repository" class="md-source" data-md-source="github">
    
      <div class="md-source__icon">
        <svg viewBox="0 0 24 24" width="24" height="24">
          <use xlink:href="#__github" width="24" height="24"></use>
        </svg>
      </div>
    
    <div class="md-source__repository">
      FreeFem++ on Github
    </div>
  </a>

          </div>
        </div>
      
    </div>
  </nav>
</header>
    
    <div class="md-container">
      
        
      
      
        

  

<nav class="md-tabs md-tabs--active" data-md-component="tabs">
  <div class="md-tabs__inner md-grid">
    <ul class="md-tabs__list">
      
        
  
  
    <li class="md-tabs__item">
      
        <a href="../.." title="Introduction" class="md-tabs__link">
          Introduction
        </a>
      
    </li>
  

      
        
  
  
    <li class="md-tabs__item">
      
        <a href="../" title="Documentation" class="md-tabs__link md-tabs__link--active">
          Documentation
        </a>
      
    </li>
  

      
        
  
  
    <li class="md-tabs__item">
      
        <a href="../../reference/" title="Language References" class="md-tabs__link">
          Language References
        </a>
      
    </li>
  

      
        
  
  
    <li class="md-tabs__item">
      
        <a href="../../tutorials/" title="Tutorials" class="md-tabs__link">
          Tutorials
        </a>
      
    </li>
  

      
        
  
  
    <li class="md-tabs__item">
      
        <a href="../../examples/" title="Examples" class="md-tabs__link">
          Examples
        </a>
      
    </li>
  

      
        
  
  
    <li class="md-tabs__item">
      
        <a href="../../models/" title="Models" class="md-tabs__link">
          Models
        </a>
      
    </li>
  

      
    </ul>
  </div>
</nav>
      
      <main class="md-main">
        <div class="md-main__inner md-grid" data-md-component="container">
          
            
              <div class="md-sidebar md-sidebar--primary" data-md-component="navigation">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    <nav class="md-nav md-nav--primary" data-md-level="0">
  <label class="md-nav__title md-nav__title--site" for="__drawer">
    <a href="../.." title="FreeFem++" class="md-nav__button md-logo">
      
        <img src="../../images/favicon.png" width="48" height="48">
      
    </a>
    FreeFem++
  </label>
  
    <div class="md-nav__source">
      


  


  <a href="https://github.com/FreeFem/FreeFem-doc/" title="Go to repository" class="md-source" data-md-source="github">
    
      <div class="md-source__icon">
        <svg viewBox="0 0 24 24" width="24" height="24">
          <use xlink:href="#__github" width="24" height="24"></use>
        </svg>
      </div>
    
    <div class="md-source__repository">
      FreeFem++ on Github
    </div>
  </a>

    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-1" type="checkbox" id="nav-1">
    
    <label class="md-nav__link" for="nav-1">
      Introduction
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-1">
        Introduction
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../.." title="Introduction" class="md-nav__link">
      Introduction
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../introduction/download/" title="Download" class="md-nav__link">
      Download
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../introduction/installation/" title="Installation" class="md-nav__link">
      Installation
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../introduction/contributing/" title="How to contribute?" class="md-nav__link">
      How to contribute?
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../introduction/citation/" title="Citation" class="md-nav__link">
      Citation
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../introduction/authors/" title="Authors" class="md-nav__link">
      Authors
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../introduction/TODO/" title="TODO" class="md-nav__link">
      TODO
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      

  


  <li class="md-nav__item md-nav__item--active md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-2" type="checkbox" id="nav-2" checked>
    
    <label class="md-nav__link" for="nav-2">
      Documentation
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-2">
        Documentation
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../" title="Home" class="md-nav__link">
      Home
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../Notations/" title="Some Notations" class="md-nav__link">
      Some Notations
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../MeshGeneration/" title="Mesh Generation" class="md-nav__link">
      Mesh Generation
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../FiniteElement/" title="Finite Element" class="md-nav__link">
      Finite Element
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../Visualization/" title="Visualization" class="md-nav__link">
      Visualization
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../AlgorithmsOptimization/" title="Algorithms & Optimization" class="md-nav__link">
      Algorithms & Optimization
    </a>
  </li>

        
          
          
          

  


  <li class="md-nav__item md-nav__item--active">
    
    <input class="md-toggle md-nav__toggle" data-md-toggle="toc" type="checkbox" id="__toc">
    
    
      <label class="md-nav__link md-nav__link--active" for="__toc">
        Parallelization
      </label>
    
    <a href="./" title="Parallelization" class="md-nav__link md-nav__link--active">
      Parallelization
    </a>
    
      
<nav class="md-nav md-nav--secondary">
  
  
  
    <label class="md-nav__title" for="__toc">Table of contents</label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#mpi" title="MPI" class="md-nav__link">
    MPI
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mpi-keywords" title="MPI Keywords" class="md-nav__link">
    MPI Keywords
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mpi-constants" title="MPI Constants" class="md-nav__link">
    MPI Constants
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mpi-constructor" title="MPI Constructor" class="md-nav__link">
    MPI Constructor
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mpi-functions" title="MPI Functions" class="md-nav__link">
    MPI Functions
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mpi-communicator-operator" title="MPI Communicator operator" class="md-nav__link">
    MPI Communicator operator
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#schwarz-example-in-parallel" title="Schwarz example in parallel" class="md-nav__link">
    Schwarz example in parallel
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#true-parallel-schwarz-example" title="True parallel Schwarz example" class="md-nav__link">
    True parallel Schwarz example
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#parallel-sparse-solvers" title="Parallel sparse solvers" class="md-nav__link">
    Parallel sparse solvers
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#using-parallel-sparse-solvers-in-freefem" title="Using parallel sparse solvers in FreeFem++" class="md-nav__link">
    Using parallel sparse solvers in FreeFem++
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse-direct-solver" title="Sparse direct solver" class="md-nav__link">
    Sparse direct solver
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mumps-solver" title="MUMPS solver" class="md-nav__link">
    MUMPS solver
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#superlu-distributed-solver" title="SuperLU distributed solver" class="md-nav__link">
    SuperLU distributed solver
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pastix-solver" title="PaStiX solver" class="md-nav__link">
    PaStiX solver
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#parallel-sparse-iterative-solver" title="Parallel sparse iterative solver" class="md-nav__link">
    Parallel sparse iterative solver
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parms-solver" title="pARMS solver" class="md-nav__link">
    pARMS solver
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#interfacing-with-hips" title="Interfacing with HIPS" class="md-nav__link">
    Interfacing with HIPS
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#interfacing-with-hypre" title="Interfacing with HYPRE" class="md-nav__link">
    Interfacing with HYPRE
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#conclusion" title="Conclusion" class="md-nav__link">
    Conclusion
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#domain-decomposition" title="Domain decomposition" class="md-nav__link">
    Domain decomposition
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#communicators-and-groups" title="Communicators and groups" class="md-nav__link">
    Communicators and groups
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#process" title="Process" class="md-nav__link">
    Process
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#points-to-points-communicators" title="Points to Points communicators" class="md-nav__link">
    Points to Points communicators
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#global-operations" title="Global operations" class="md-nav__link">
    Global operations
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#hpddm-solvers" title="HPDDM solvers" class="md-nav__link">
    HPDDM solvers
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#time-dependent-problem" title="Time dependent problem" class="md-nav__link">
    Time dependent problem
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#distributed-vectors-in-hpddm" title="Distributed vectors in HPDDM" class="md-nav__link">
    Distributed vectors in HPDDM
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#references" title="References" class="md-nav__link">
    References
  </a>
  
</li>
      
      
      
      
      
    </ul>
  
</nav>
    
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../Plugins/" title="Plugins" class="md-nav__link">
      Plugins
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../Developers/" title="Developers" class="md-nav__link">
      Developers
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../TODO/" title="TODO" class="md-nav__link">
      TODO
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-3" type="checkbox" id="nav-3">
    
    <label class="md-nav__link" for="nav-3">
      Language References
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-3">
        Language References
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../reference/" title="Home" class="md-nav__link">
      Home
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../reference/Types/" title="Types" class="md-nav__link">
      Types
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../reference/GlobalVariables/" title="Global variables" class="md-nav__link">
      Global variables
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../reference/QuadratureFormulae/" title="Quadrature formulae" class="md-nav__link">
      Quadrature formulae
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../reference/Operators/" title="Operators" class="md-nav__link">
      Operators
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../reference/Loops/" title="Loops" class="md-nav__link">
      Loops
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../reference/IO/" title="I/O" class="md-nav__link">
      I/O
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../reference/Functions/" title="Functions" class="md-nav__link">
      Functions
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../reference/ExternalLibraries/" title="External libraries" class="md-nav__link">
      External libraries
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../reference/TODO/" title="TODO" class="md-nav__link">
      TODO
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-4" type="checkbox" id="nav-4">
    
    <label class="md-nav__link" for="nav-4">
      Tutorials
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-4">
        Tutorials
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../tutorials/" title="Getting started" class="md-nav__link">
      Getting started
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../tutorials/Poisson/" title="Poisson's equation" class="md-nav__link">
      Poisson's equation
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../tutorials/EquationsClassification/" title="Classification of the equations" class="md-nav__link">
      Classification of the equations
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../tutorials/Membrane/" title="Membrane" class="md-nav__link">
      Membrane
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../tutorials/HeatExchanger/" title="Heat Exchanger" class="md-nav__link">
      Heat Exchanger
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../tutorials/Acoustics/" title="Acoustics" class="md-nav__link">
      Acoustics
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../tutorials/ThermalConduction/" title="Thermal Conduction" class="md-nav__link">
      Thermal Conduction
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../tutorials/FanBlade/" title="Irrotational Fan Blade Flow and Thermal effects" class="md-nav__link">
      Irrotational Fan Blade Flow and Thermal effects
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../tutorials/RotatingHill/" title="Pure convection, The rotating hill" class="md-nav__link">
      Pure convection, The rotating hill
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../tutorials/Elasticity/" title="The system of elasticity" class="md-nav__link">
      The system of elasticity
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../tutorials/Stokes/" title="The system of Stokes for fluids" class="md-nav__link">
      The system of Stokes for fluids
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../tutorials/NavierStokesProjection/" title="A projection Algorithm for the Navier-Stokes equations" class="md-nav__link">
      A projection Algorithm for the Navier-Stokes equations
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../tutorials/NavierStokesNewton/" title="Newton method for the steady Navier-Stokes equations" class="md-nav__link">
      Newton method for the steady Navier-Stokes equations
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../tutorials/ALargeFluidProblem/" title="A large fluid problem" class="md-nav__link">
      A large fluid problem
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../tutorials/ComplexNumbers/" title="An example with complex numbers" class="md-nav__link">
      An example with complex numbers
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../tutorials/OptimalControl/" title="Optimal control" class="md-nav__link">
      Optimal control
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../tutorials/FlowWithShocks/" title="A flow with shocks" class="md-nav__link">
      A flow with shocks
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../tutorials/HeatEquationOptimization/" title="Time dependent schema optimization for heat equations" class="md-nav__link">
      Time dependent schema optimization for heat equations
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../tutorials/TimeDependentStokes/" title="A transient Stokes solver in matrix form" class="md-nav__link">
      A transient Stokes solver in matrix form
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../tutorials/WifiPropagation/" title="Wifi Propagation" class="md-nav__link">
      Wifi Propagation
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../tutorials/MatlabOctavePlot/" title="Matlab / Octave Plots" class="md-nav__link">
      Matlab / Octave Plots
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../tutorials/TODO/" title="TODO" class="md-nav__link">
      TODO
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-5" type="checkbox" id="nav-5">
    
    <label class="md-nav__link" for="nav-5">
      Examples
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-5">
        Examples
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../examples/" title="Home" class="md-nav__link">
      Home
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../examples/TODO/" title="TODO" class="md-nav__link">
      TODO
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-6" type="checkbox" id="nav-6">
    
    <label class="md-nav__link" for="nav-6">
      Models
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-6">
        Models
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../models/" title="Home" class="md-nav__link">
      Home
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../models/StaticProblems/" title="Static problems" class="md-nav__link">
      Static problems
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../models/Elasticity/" title="Elasticity" class="md-nav__link">
      Elasticity
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../models/NonLinearStaticProblems/" title="Non-linear static problems" class="md-nav__link">
      Non-linear static problems
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../models/EigenValueProblems/" title="Eigenvalue problems" class="md-nav__link">
      Eigenvalue problems
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../models/EvolutionProblems/" title="Evolution problems" class="md-nav__link">
      Evolution problems
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../models/NavierStokesEquations/" title="Navier-Stokes equations" class="md-nav__link">
      Navier-Stokes equations
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../models/VariationalInequality/" title="Variational inequality" class="md-nav__link">
      Variational inequality
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../models/DomainDecomposition/" title="Domain decomposition" class="md-nav__link">
      Domain decomposition
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../models/FluidStructureCoupledProblem/" title="Fluid-Structure coupled problem" class="md-nav__link">
      Fluid-Structure coupled problem
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../models/TransmissionProblem/" title="Transmission problem" class="md-nav__link">
      Transmission problem
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../models/FreeBoundaryProblem/" title="Free boundary problem" class="md-nav__link">
      Free boundary problem
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../models/NonLinearElasticity/" title="Non-linear elasticity" class="md-nav__link">
      Non-linear elasticity
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../models/CompressibleNeoHookeanMaterials/" title="Compressible Neo-Hookean materials" class="md-nav__link">
      Compressible Neo-Hookean materials
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../models/WhisperingGalleryModes/" title="Whispering gallery modes" class="md-nav__link">
      Whispering gallery modes
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../models/TODO/" title="TODO" class="md-nav__link">
      TODO
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              <div class="md-sidebar md-sidebar--secondary" data-md-component="toc">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
<nav class="md-nav md-nav--secondary">
  
  
  
    <label class="md-nav__title" for="__toc">Table of contents</label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#mpi" title="MPI" class="md-nav__link">
    MPI
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mpi-keywords" title="MPI Keywords" class="md-nav__link">
    MPI Keywords
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mpi-constants" title="MPI Constants" class="md-nav__link">
    MPI Constants
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mpi-constructor" title="MPI Constructor" class="md-nav__link">
    MPI Constructor
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mpi-functions" title="MPI Functions" class="md-nav__link">
    MPI Functions
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mpi-communicator-operator" title="MPI Communicator operator" class="md-nav__link">
    MPI Communicator operator
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#schwarz-example-in-parallel" title="Schwarz example in parallel" class="md-nav__link">
    Schwarz example in parallel
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#true-parallel-schwarz-example" title="True parallel Schwarz example" class="md-nav__link">
    True parallel Schwarz example
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#parallel-sparse-solvers" title="Parallel sparse solvers" class="md-nav__link">
    Parallel sparse solvers
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#using-parallel-sparse-solvers-in-freefem" title="Using parallel sparse solvers in FreeFem++" class="md-nav__link">
    Using parallel sparse solvers in FreeFem++
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse-direct-solver" title="Sparse direct solver" class="md-nav__link">
    Sparse direct solver
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mumps-solver" title="MUMPS solver" class="md-nav__link">
    MUMPS solver
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#superlu-distributed-solver" title="SuperLU distributed solver" class="md-nav__link">
    SuperLU distributed solver
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pastix-solver" title="PaStiX solver" class="md-nav__link">
    PaStiX solver
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#parallel-sparse-iterative-solver" title="Parallel sparse iterative solver" class="md-nav__link">
    Parallel sparse iterative solver
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parms-solver" title="pARMS solver" class="md-nav__link">
    pARMS solver
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#interfacing-with-hips" title="Interfacing with HIPS" class="md-nav__link">
    Interfacing with HIPS
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#interfacing-with-hypre" title="Interfacing with HYPRE" class="md-nav__link">
    Interfacing with HYPRE
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#conclusion" title="Conclusion" class="md-nav__link">
    Conclusion
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#domain-decomposition" title="Domain decomposition" class="md-nav__link">
    Domain decomposition
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#communicators-and-groups" title="Communicators and groups" class="md-nav__link">
    Communicators and groups
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#process" title="Process" class="md-nav__link">
    Process
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#points-to-points-communicators" title="Points to Points communicators" class="md-nav__link">
    Points to Points communicators
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#global-operations" title="Global operations" class="md-nav__link">
    Global operations
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#hpddm-solvers" title="HPDDM solvers" class="md-nav__link">
    HPDDM solvers
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#time-dependent-problem" title="Time dependent problem" class="md-nav__link">
    Time dependent problem
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#distributed-vectors-in-hpddm" title="Distributed vectors in HPDDM" class="md-nav__link">
    Distributed vectors in HPDDM
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#references" title="References" class="md-nav__link">
    References
  </a>
  
</li>
      
      
      
      
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content">
            <article class="md-content__inner md-typeset">
              
                
                  <a href="https://github.com/FreeFem/FreeFem-doc/edit/master/docs/documentation/Parallelization.md" title="Edit this page" class="md-icon md-content__icon">&#xE3C9;</a>
                
                
                  <h1>Parallelization</h1>
                
                <p>A first attempt of parallelization of FreeFem++ is made here with <strong><code class="codehilite">mpi</code></strong>. An extended interface with MPI has been added to FreeFem++ version 3.5, (see the <a href="http://mpi-forum.org/docs/">MPI documentation</a> for the functionality of the language).</p>
<h2 id="mpi">MPI<a class="headerlink" href="#mpi" title="Permanent link">#</a></h2>
<h3 id="mpi-keywords">MPI Keywords<a class="headerlink" href="#mpi-keywords" title="Permanent link">#</a></h3>
<p>The following keywords and concepts are used:</p>
<ul>
<li><code class="codehilite">mpiComm</code> to defined a <em>communication world</em></li>
<li><code class="codehilite">mpiGroup</code> to defined a group of <em>processors</em> in the communication world</li>
<li><code class="codehilite">mpiRequest</code> to defined a request to wait for the end of the communication</li>
</ul>
<h3 id="mpi-constants">MPI Constants<a class="headerlink" href="#mpi-constants" title="Permanent link">#</a></h3>
<ul>
<li><code class="codehilite">mpisize</code> The total number of <em>processes</em>,</li>
<li><code class="codehilite">mpirank</code> the id-number of my current process in <code>{0, ..., mpisize-1}</code>,</li>
<li><code class="codehilite">mpiUndefined</code> The <code class="codehilite"><span class="n">MPI_Undefined</span></code> constant,</li>
<li><code class="codehilite">mpiAnySource</code> The <code class="codehilite"><span class="n">MPI_ANY_SOURCE</span></code> constant,</li>
<li><code class="codehilite">mpiCommWorld</code> The <code class="codehilite"><span class="n">MPI_COMM_WORLD</span></code> constant,</li>
<li>[ ... ] and all the keywords of <code class="codehilite">MPI_Op</code> for the <em>reduce</em> operator:
    <code class="codehilite">mpiMAX</code>, <code class="codehilite">mpiMIN</code>, <code class="codehilite">mpiSUM</code>, <code class="codehilite">mpiPROD</code>, <code class="codehilite">mpiLAND</code>, <code class="codehilite">mpiLOR</code>, <code class="codehilite">mpiLXOR</code>, <code class="codehilite">mpiBAND</code>, <code class="codehilite">mpiBXOR</code>.</li>
</ul>
<h3 id="mpi-constructor">MPI Constructor<a class="headerlink" href="#mpi-constructor" title="Permanent link">#</a></h3>
<div class="codehilite"><pre><span></span>// Parameters
int[int] proc1 = [1, 2], proc2 = [0, 3];
int color = 1;
int key = 1;

// MPI ranks
cout &lt;&lt; &quot;MPI rank = &quot; &lt;&lt; mpirank &lt;&lt; endl;

// MPI
mpiComm comm(mpiCommWorld, 0, 0); //set a MPI_Comm to MPI_COMM_WORLD

mpiGroup grp(proc1); //set MPI_Group to proc 1,2 in MPI_COMM_WORLD
mpiGroup grp1(comm, proc1); //set MPI_Group to proc 1,2 in comm

mpiComm ncomm1(mpiCommWorld, grp); //set the MPI_Comm form grp

mpiComm ncomm2(comm, color, key); //MPI_Comm_split(MPI_Comm comm, int color, int key, MPI_Comm *ncomm)

mpiRequest rq; //defined an MPI_Request
mpiRequest[int] arq(10); //defined an array of 10 MPI_Request
</pre></div>

<h3 id="mpi-functions">MPI Functions<a class="headerlink" href="#mpi-functions" title="Permanent link">#</a></h3>
<div class="codehilite"><pre><span></span>mpiComm Comm(mpiCommWorld, 0, 0);

int MPICommSize = mpiSize(Comm);
int MPIRank = mpiRank(Comm);

if (MPIRank == 0) cout &lt;&lt; &quot;MPI Comm size = &quot; &lt;&lt; MPICommSize &lt;&lt; endl;
cout &lt;&lt; &quot;MPI rank in Comm = &quot; &lt;&lt; mpiRank(Comm) &lt;&lt; endl;

mpiRequest Req;
mpiRequest[int] ReqArray(10);

for (int i = 0; i &lt; MPICommSize; i++){
     //return processor i with no Resquest in MPI_COMM_WORLD
    processor(i);
    //return processor any source with no Resquest in MPI_COMM_WORLD
    processor(mpiAnySource);
    //return processor i with no Resquest in Comm
    processor(i, Comm);
    //return processor i with no Resquest in Comm
    processor(Comm, i);
    //return processor i with Resquest rq in Comm
    /* processor(i, Req, Comm);
    //return processor i with Resquest rq in MPI_COMM_WORLD
    processor(i, Req); */
    //return processor i in MPI_COMM_WORLD in block mode for synchronously communication
    processorblock(i);
    //return processor any source in MPI_COMM_WORLD in block mode for synchronously communication
    processorblock(mpiAnySource);
    //return processor i in in Comm in block mode
    processorblock(i, Comm);
}

mpiBarrier(Comm); //do a MPI_Barrier on communicator Comm
mpiWaitAny(ReqArray); //wait add of Request array,
mpiWait(Req); //wait on a Request
real t = mpiWtime(); //return MPIWtime in second
real tick = mpiWtick(); //return MPIWTick in second
</pre></div>

<p>where a <code class="codehilite">processor</code> is just a integer rank, pointer to a <code class="codehilite"><span class="n">MPI_comm</span></code> and pointer to a <code class="codehilite"><span class="n">MPI_Request</span></code>, and <code class="codehilite">processorblock</code> with a special <code class="codehilite"><span class="n">MPI_Request</span></code>.</p>
<h3 id="mpi-communicator-operator">MPI Communicator operator<a class="headerlink" href="#mpi-communicator-operator" title="Permanent link">#</a></h3>
<p><div class="codehilite"><pre><span></span>int status; //to get the MPI status of send / recv
real a, b;

mpiComm comm(mpiCommWorld, 0, 0);
mpiRequest req;

//send a,b asynchronously to the process 1
processor(1) &lt;&lt; a &lt;&lt; b;
//receive a,b synchronously from the process 10
processor(10) &gt;&gt; a &gt;&gt; b;

//broadcast from processor of comm to other comm processor
// broadcast(processor(10, comm), a);
//send synchronously to the process 10 the data a
status = Send(processor(10, comm), a);
//receive synchronously from the process 10 the data a
status = Recv(processor(10, comm), a);

//send asynchronously to the process 10 the data a without request
status = Isend(processor(10, comm), a);
//send asynchronously to the process 10 the data a with request
status = Isend(processor(10, comm, req), a);
//receive asynchronously from the process 10 the data a
status = Irecv(processor(10, req), a);
//Error asynchronously without request.
// status = Irecv(processor(10), a);
</pre></div>
where the data type of <code class="codehilite">a</code> can be of type of <code class="codehilite">int</code>,<code class="codehilite">real</code>, <code class="codehilite">complex</code>, <code class="codehilite">int[int]</code>, <code class="codehilite">real[int]</code>, <code class="codehilite">complex[int]</code>, <code class="codehilite">int[int,int]</code>, <code class="codehilite">double[int,int]</code>, <code class="codehilite">complex[int,int]</code>, <code class="codehilite">mesh</code>, <code class="codehilite">mesh3</code>, <code class="codehilite">mesh[int]</code>, <code class="codehilite">mesh3[int]</code>, <code class="codehilite">matrix</code>, <code class="codehilite">matrix&lt;complex&gt;</code></p>
<div class="codehilite"><pre><span></span>//send asynchronously to the process 10 the data a with request
processor(10, req) &lt;&lt; a ;
//receive asynchronously from the process 10 the data a with request
processor(10, req) &gt;&gt; a ;
</pre></div>

<p>If <code class="codehilite">a, b</code> are arrays or full matrices of <code class="codehilite">int</code>, <code class="codehilite">real</code>, or <code class="codehilite">complex</code>, we can use the following MPI functions:</p>
<p><div class="codehilite"><pre><span></span>mpiAlltoall(a, b, [comm]);
mpiAllgather(a, b, [comm]);
mpiGather(a, b, processor(..) );
mpiScatter(a, b, processor(..));
mpiReduce(a, b, processor(..), mpiMAX);
mpiAllReduce(a, b, comm, mpiMAX);
</pre></div>
Thank you to Guy-Antoine Atenekeng Kahou for his help to code this interface.</p>
<h3 id="schwarz-example-in-parallel">Schwarz example in parallel<a class="headerlink" href="#schwarz-example-in-parallel" title="Permanent link">#</a></h3>
<p>This example is a rewritting of example <a href="../models/DomainDecomposition/#schwarz-overlapping">Schwarz overlapping</a>.</p>
<div class="codehilite"><pre><span></span>ff-mpirun -np <span class="m">2</span> SchwarzParallel.edp
<span class="c1"># OR</span>
mpirun -np <span class="m">2</span> FreeFem++-mpi SchwarzParallel.edp
</pre></div>

<p><div class="codehilite"><pre><span></span>if (mpisize != 2){
    cout &lt;&lt; &quot; sorry, number of processors !=2 &quot; &lt;&lt; endl;
    exit(1);
}

// Parameters
verbosity = 0;
int interior = 2;
int exterior = 1;
int n = 4;

// Mesh
border a(t=1, 2){x=t; y=0; label=exterior;}
border b(t=0, 1){x=2; y=t; label=exterior;}
border c(t=2, 0){x=t; y=1; label=exterior;}
border d(t=1, 0){x=1-t; y=t; label=interior;}
border e(t=0, pi/2){x=cos(t); y=sin(t); label=interior;}
border e1(t=pi/2, 2*pi){x=cos(t); y=sin(t); label=exterior;}
mesh[int] Th(mpisize);
if (mpirank == 0)
    Th[0] = buildmesh(a(5*n) + b(5*n) + c(10*n) + d(5*n));
else
    Th[1] = buildmesh(e(5*n) + e1(25*n));

broadcast(processor(0), Th[0]);
broadcast(processor(1), Th[1]);

// Fespace
fespace Vh(Th[mpirank], P1);
Vh u = 0, v;

fespace Vhother(Th[1-mpirank], P1);
Vhother U = 0;

//Problem
int i = 0;
problem pb (u, v, init=i, solver=Cholesky)
    = int2d(Th[mpirank])(
          dx(u)*dx(v)
        + dy(u)*dy(v)
    )
    - int2d(Th[mpirank])(
          v
    )
    + on(interior, u=U)
    + on(exterior, u= 0 )
    ;

// Loop
for (i = 0; i &lt; 20; i++){
    cout &lt;&lt; mpirank &lt;&lt; &quot; - Loop &quot; &lt;&lt; i &lt;&lt; endl;

    // Solve
    pb;
    //send u to the other proc, receive in U
    processor(1-mpirank) &lt;&lt; u[];
    processor(1-mpirank) &gt;&gt; U[];

    // Error
    real err0, err1;
    err0 = int1d(Th[mpirank],interior)(square(U - u));
    // send err0 to the other proc, receive in err1
    processor(1-mpirank) &lt;&lt; err0;
    processor(1-mpirank) &gt;&gt; err1;
    real err = sqrt(err0 + err1);
    cout &lt;&lt; &quot; err = &quot; &lt;&lt; err &lt;&lt; &quot; - err0 = &quot; &lt;&lt; err0 &lt;&lt; &quot; - err1 = &quot; &lt;&lt; err1 &lt;&lt; endl;
    if (err &lt; 1e-3) break;
}
if (mpirank == 0)
    plot(u, U);
</pre></div>
<span><span class="MathJax_Preview">\codered</span><script type="math/tex">\codered</script></span> script freeze in the loop</p>
<h4 id="true-parallel-schwarz-example">True parallel Schwarz example<a class="headerlink" href="#true-parallel-schwarz-example" title="Permanent link">#</a></h4>
<p><em>Thank you to F. Nataf</em></p>
<p>This is a explanation of the two examples <a href="../examples/#mpi-gmres-2d">MPI-GMRES 2D</a> and <a href="../examples/#mpi-gmres-3d">MPI-GMRES 3D</a>, a Schwarz parallel with a complexity almost independent of the number of process (with a coarse grid preconditioner).</p>
<p>To solve the following Poisson problem on domain <span><span class="MathJax_Preview">\Omega</span><script type="math/tex">\Omega</script></span> with boundary <span><span class="MathJax_Preview">\Gamma</span><script type="math/tex">\Gamma</script></span> in <span><span class="MathJax_Preview">L^2(\Omega)</span><script type="math/tex">L^2(\Omega)</script></span> :</p>
<div>
<div class="MathJax_Preview">\begin{eqnarray}
    -\Delta u &amp;=&amp; f &amp; \mbox{ in } \Omega\\
    u &amp;=&amp; g &amp; \mbox{ on } \Gamma
\end{eqnarray}</div>
<script type="math/tex; mode=display">\begin{eqnarray}
    -\Delta u &=& f & \mbox{ in } \Omega\\
    u &=& g & \mbox{ on } \Gamma
\end{eqnarray}</script>
</div>
<p>where <span><span class="MathJax_Preview">f</span><script type="math/tex">f</script></span> and <span><span class="MathJax_Preview">g</span><script type="math/tex">g</script></span> are two given functions of <span><span class="MathJax_Preview">L^2(\Omega)</span><script type="math/tex">L^2(\Omega)</script></span> and of <span><span class="MathJax_Preview">H^{\frac12}(\Gamma)</span><script type="math/tex">H^{\frac12}(\Gamma)</script></span>,</p>
<p>Lets introduce <span><span class="MathJax_Preview">(\pi_i)_{i=1,.., N_p}</span><script type="math/tex">(\pi_i)_{i=1,.., N_p}</script></span> a regular partition of the unity of <span><span class="MathJax_Preview">\Omega</span><script type="math/tex">\Omega</script></span>, q-e-d:
<!--- __ --->
<script type="math/tex; mode=display">
\pi_i \in \mathcal{C}^0(\Omega) : \quad \pi_i\ge 0 \mbox{ and } \sum_{i=1}^{N_p} \pi_i =1 .
</script>
</p>
<p>Denote <span><span class="MathJax_Preview">\Omega_i</span><script type="math/tex">\Omega_i</script></span> the sub domain which is the support of <span><span class="MathJax_Preview">\pi_i</span><script type="math/tex">\pi_i</script></span> function and also denote <span><span class="MathJax_Preview">\Gamma_i</span><script type="math/tex">\Gamma_i</script></span> the boundary of <span><span class="MathJax_Preview">\Omega_i</span><script type="math/tex">\Omega_i</script></span>.</p>
<p>The parallel Schwarz method is:</p>
<p>Let <span><span class="MathJax_Preview">\ell=0</span><script type="math/tex">\ell=0</script></span> the iterator and a initial guest <span><span class="MathJax_Preview">u^0</span><script type="math/tex">u^0</script></span> respecting the boundary condition (i.e. <span><span class="MathJax_Preview">u^0_{|\Gamma} = g</span><script type="math/tex">u^0_{|\Gamma} = g</script></span>).</p>
<div>
<div class="MathJax_Preview">\begin{eqnarray}
    \forall i = 1 .., N_p:&amp;\nonumber\\
    \displaystyle -\Delta u_i^\ell &amp;=&amp; f &amp;\mbox{ in } \Omega_i\label{eq:lapl}\\
    u_i^\ell &amp;=&amp; u^\ell &amp; \mbox{ on } \Gamma_i \setminus \Gamma\\
    u_i^\ell &amp;=&amp; g &amp; \mbox{ on } \Gamma_i \cap \Gamma
\end{eqnarray}</div>
<script type="math/tex; mode=display">\begin{eqnarray}
    \forall i = 1 .., N_p:&\nonumber\\
    \displaystyle -\Delta u_i^\ell &=& f &\mbox{ in } \Omega_i\label{eq:lapl}\\
    u_i^\ell &=& u^\ell & \mbox{ on } \Gamma_i \setminus \Gamma\\
    u_i^\ell &=& g & \mbox{ on } \Gamma_i \cap \Gamma
\end{eqnarray}</script>
</div>
<div>
<div class="MathJax_Preview">\begin{equation}
\label{eq:pu1}
u^{\ell+1} = \sum_{i=1}^{N_p} \pi_i u_i^\ell
\end{equation}</div>
<script type="math/tex; mode=display">\begin{equation}
\label{eq:pu1}
u^{\ell+1} = \sum_{i=1}^{N_p} \pi_i u_i^\ell
\end{equation}</script>
</div>
<p>After discretization with the Lagrange finite element method, with a compatible mesh <span><span class="MathJax_Preview">{\mathcal{T}_h}_i</span><script type="math/tex">{\mathcal{T}_h}_i</script></span> of <span><span class="MathJax_Preview">\Omega_i</span><script type="math/tex">\Omega_i</script></span>, i. e., the exist a global mesh <span><span class="MathJax_Preview">{\mathcal{T}_h}</span><script type="math/tex">{\mathcal{T}_h}</script></span> such that <span><span class="MathJax_Preview">{\mathcal{T}_h}_i</span><script type="math/tex">{\mathcal{T}_h}_i</script></span> is include in <span><span class="MathJax_Preview">{\mathcal{T}_h}</span><script type="math/tex">{\mathcal{T}_h}</script></span>.</p>
<p>Let us denote:</p>
<ul>
<li><span><span class="MathJax_Preview">{V_h}_i</span><script type="math/tex">{V_h}_i</script></span> the finite element space corresponding to domain <span><span class="MathJax_Preview">\Omega_i</span><script type="math/tex">\Omega_i</script></span>,</li>
<li><span><span class="MathJax_Preview">{\mathcal{N}_h}_i</span><script type="math/tex">{\mathcal{N}_h}_i</script></span> is the set of the degree of freedom <span><span class="MathJax_Preview">\sigma_i^k</span><script type="math/tex">\sigma_i^k</script></span>,</li>
<li><span><span class="MathJax_Preview">{\mathcal{N}^{\Gamma_i}_{hi}}</span><script type="math/tex">{\mathcal{N}^{\Gamma_i}_{hi}}</script></span> is the set of the degree of freedom of <span><span class="MathJax_Preview">{V_h}_i</span><script type="math/tex">{V_h}_i</script></span> on the boundary <span><span class="MathJax_Preview">\Gamma_i</span><script type="math/tex">\Gamma_i</script></span> of <span><span class="MathJax_Preview">\Omega_i</span><script type="math/tex">\Omega_i</script></span>,</li>
<li><span><span class="MathJax_Preview">\sigma_i^k({v_h})</span><script type="math/tex">\sigma_i^k({v_h})</script></span> is the value the degree of freedom <span><span class="MathJax_Preview">k</span><script type="math/tex">k</script></span>,</li>
<li><span><span class="MathJax_Preview">{V_{0h}}_i= \{ {v_h} \in {V_h}_i :\forall k \in {\mathcal{N}^{\Gamma_i}_{hi}}, \quad \sigma_i^k({v_h})=0 \}</span><script type="math/tex">{V_{0h}}_i= \{ {v_h} \in {V_h}_i :\forall k \in {\mathcal{N}^{\Gamma_i}_{hi}}, \quad \sigma_i^k({v_h})=0 \}</script></span>,</li>
<li>
<p>The conditional expression <span><span class="MathJax_Preview">a\;?\;b:c</span><script type="math/tex">a\;?\;b:c</script></span> is defined like in <code class="codehilite"><span class="n">C</span></code> of <code class="codehilite"><span class="n">C</span><span class="o">++</span></code> language by</p>
<div>
<div class="MathJax_Preview">
a?b: c \equiv
\left\{
\begin{array}{l}
\mbox{if $a$ is true then return $b$}\\
\mbox{else return $c$}\\
\end{array}
\right..
</div>
<script type="math/tex; mode=display">
a?b: c \equiv
\left\{
\begin{array}{l}
\mbox{if $a$ is true then return $b$}\\
\mbox{else return $c$}\\
\end{array}
\right..
</script>
</div>
</li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>We never use finite element space associated to the full domain <span><span class="MathJax_Preview">\Omega</span><script type="math/tex">\Omega</script></span> because it is too expensive.</p>
</div>
<p>We have to defined to operator to build the previous algorithm:</p>
<p>We denote <span><span class="MathJax_Preview">{u_h^{\ell}}_{|i}</span><script type="math/tex">{u_h^{\ell}}_{|i}</script></span> the restriction of <span><span class="MathJax_Preview">u_h^\ell</span><script type="math/tex">u_h^\ell</script></span> on <span><span class="MathJax_Preview">{V_h}_i</span><script type="math/tex">{V_h}_i</script></span>, so the discrete problem on <span><span class="MathJax_Preview">\Omega_i</span><script type="math/tex">\Omega_i</script></span> of problem \eqref{eq:lapl} is find <span><span class="MathJax_Preview">{u_h^{\ell}}_{i}\in {V_h}_i</span><script type="math/tex">{u_h^{\ell}}_{i}\in {V_h}_i</script></span> such that:</p>
<div>
<div class="MathJax_Preview">\begin{equation}
\forall {v_h}_i\in V_{0i}:
\int_{\Omega_i} \nabla {v_h}_i \cdot \nabla {u_h}^{\ell}_{i}
= \int_{\Omega_i} f {v_h}_i ,\quad \forall k \in {\mathcal{N}^{\Gamma_i}_{hi}}\;:\; \sigma_i^k({u_h}^\ell_i) = (k\in \Gamma) \; ? \; g_i^k : \sigma_i^k({u_h}^{\ell}_{|i})
\end{equation}</div>
<script type="math/tex; mode=display">\begin{equation}
\forall {v_h}_i\in V_{0i}:
\int_{\Omega_i} \nabla {v_h}_i \cdot \nabla {u_h}^{\ell}_{i}
= \int_{\Omega_i} f {v_h}_i ,\quad \forall k \in {\mathcal{N}^{\Gamma_i}_{hi}}\;:\; \sigma_i^k({u_h}^\ell_i) = (k\in \Gamma) \; ? \; g_i^k : \sigma_i^k({u_h}^{\ell}_{|i})
\end{equation}</script>
</div>
<p>where <span><span class="MathJax_Preview">g_i^k</span><script type="math/tex">g_i^k</script></span> is the value of <span><span class="MathJax_Preview">g</span><script type="math/tex">g</script></span> associated to the degree of freedom <span><span class="MathJax_Preview">k\in {\mathcal{N}^{\Gamma_i}_{hi}}</span><script type="math/tex">k\in {\mathcal{N}^{\Gamma_i}_{hi}}</script></span>.</p>
<p>In FreeFem++, it can be written has with <code class="codehilite">U</code> is the vector corresponding to <span><span class="MathJax_Preview">{u_h^{\ell}}_{|i}</span><script type="math/tex">{u_h^{\ell}}_{|i}</script></span> and the vector <code class="codehilite">U1</code> is the vector corresponding to <span><span class="MathJax_Preview">{u_h^{\ell}}_{i}</span><script type="math/tex">{u_h^{\ell}}_{i}</script></span> is the solution of:</p>
<div class="codehilite"><pre><span></span>real[int] U1(Ui.n);
real[int] b = onG .* U;
b = onG ? b : Bi ;
U1 = Ai^-1*b;
</pre></div>

<p>where <span><span class="MathJax_Preview">\mathtt{onG}[i] =(i \in \Gamma_i\setminus\Gamma) ? 1 : 0</span><script type="math/tex">\mathtt{onG}[i] =(i \in \Gamma_i\setminus\Gamma) ? 1 : 0</script></span>, and <span><span class="MathJax_Preview">\mathtt{Bi}</span><script type="math/tex">\mathtt{Bi}</script></span> the right of side of the problem, are defined by</p>
<div class="codehilite"><pre><span></span>// Fespace
fespace Whi(Thi, P2);

// Problem
varf vPb (U, V)
    = int3d(Thi)(
          grad(U)&#39;*grad(V)
    )
    + int3d(Thi)(
          F*V
    )
    + on(1, U=g)
    + on(10, U=G)
    ;

varf vPbon (U, V) = on(10, U=1) + on(1, U=0);

matrix Ai = vPb (Whi, Whi, solver=sparsesolver);
real[int] onG = vPbon(0, Whi);
real[int] Bi=vPb(0, Whi);
</pre></div>

<p>where the FreeFem++ label of <span><span class="MathJax_Preview">\Gamma</span><script type="math/tex">\Gamma</script></span> is 1 and the label of <span><span class="MathJax_Preview">\Gamma_i\setminus \Gamma</span><script type="math/tex">\Gamma_i\setminus \Gamma</script></span> is <span><span class="MathJax_Preview">10</span><script type="math/tex">10</script></span>.</p>
<p>To build the transfer/update part corresponding to \eqref{eq:pu1} equation on process <span><span class="MathJax_Preview">i</span><script type="math/tex">i</script></span>, let us call <code class="codehilite">njpart</code> the number the neighborhood of domain of <span><span class="MathJax_Preview">\Omega_i</span><script type="math/tex">\Omega_i</script></span> (i.e: <span><span class="MathJax_Preview">\pi_j</span><script type="math/tex">\pi_j</script></span> is none <span><span class="MathJax_Preview">0</span><script type="math/tex">0</script></span> of <span><span class="MathJax_Preview">\Omega_i</span><script type="math/tex">\Omega_i</script></span>), we store in an array <code class="codehilite">jpart</code> of size <code class="codehilite">njpart</code> all this neighborhood.</p>
<p>Let us introduce two array of matrix, <code class="codehilite">Smj[j]</code> to defined the vector to send from <span><span class="MathJax_Preview">i</span><script type="math/tex">i</script></span> to <span><span class="MathJax_Preview">j</span><script type="math/tex">j</script></span> a neighborhood process, and the matrix <span><span class="MathJax_Preview">rMj[j]</span><script type="math/tex">rMj[j]</script></span> to after to reduce owith neighborhood <span><span class="MathJax_Preview">j</span><script type="math/tex">j</script></span> domain.</p>
<p>So the tranfert and update part compute <span><span class="MathJax_Preview">v_i= \pi_i u_i + \sum_{j\in J_i} \pi_j u_j</span><script type="math/tex">v_i= \pi_i u_i + \sum_{j\in J_i} \pi_j u_j</script></span> and can be write the FreeFem++ function Update:</p>
<div class="codehilite"><pre><span></span>func bool Update (real[int] &amp;ui, real[int] &amp;vi){
    int n = jpart.n;
    for (int j = 0; j &lt; njpart; ++j) Usend[j][] = sMj[j]*ui;
    mpiRequest[int] rq(n*2);
    for (int j = 0; j &lt; n; ++j) Irecv(processor(jpart[j], comm,rq[j]), Ri[j][]);
    for (int j = 0; j &lt; n; ++j) Isend(processor(jpart[j], comm, rq[j+n]), Si[j][]);
    for (int j = 0; j &lt; n*2; ++j) int k = mpiWaitAny(rq);
    // apply the unity local partition
    vi = Pii*ui; //set to pi_i u_i
    for (int j = 0; j &lt; njpart; ++j) vi += rMj[j]*Vrecv[j][]; //add pi_j u_j
    return true;
}
</pre></div>

<p>where the buffer are defined by:</p>
<div class="codehilite"><pre><span></span>InitU(njpart, Whij, Thij, aThij, Usend) //defined the send buffer
InitU(njpart, Whij, Thij, aThij, Vrecv) //defined the revc buffer
</pre></div>

<p>with the following macro definition:</p>
<div class="codehilite"><pre><span></span>macro InitU(n, Vh, Th, aTh, U) Vh[int] U(n); for (int j = 0; j &lt; n; ++j){Th = aTh[j]; U[j] = 0;}
</pre></div>

<p><strong> First GMRES algorithm:</strong> you can easily accelerate the fixed point algorithm by using a parallel GMRES algorithm after the introduction the following affine <span><span class="MathJax_Preview">\mathcal{A}_i</span><script type="math/tex">\mathcal{A}_i</script></span> operator sub domain <span><span class="MathJax_Preview">\Omega_i</span><script type="math/tex">\Omega_i</script></span>.</p>
<div class="codehilite"><pre><span></span>func real[int] DJ0 (real[int]&amp; U){
    real[int] V(U.n), b = onG .* U;
    b = onG ? b : Bi ;
    V = Ai^-1*b;
    Update(V, U);
    V -= U;
    return V;
}
</pre></div>

<p>Where the parallel <code class="codehilite">MPIGMRES</code> or <code class="codehilite">MPICG</code> algorithm is just a simple way to solve in parallel the following <span><span class="MathJax_Preview">A_i x_i = b_i, i = 1, .., N_p</span><script type="math/tex">A_i x_i = b_i, i = 1, .., N_p</script></span> by just changing the dot product by reduce the local dot product of all process with the following MPI code:</p>
<div class="codehilite"><pre><span></span><span class="k">template</span><span class="o">&lt;</span><span class="k">class</span> <span class="nc">R</span><span class="o">&gt;</span> <span class="n">R</span> <span class="n">ReduceSum1</span><span class="p">(</span><span class="n">R</span> <span class="n">s</span><span class="p">,</span> <span class="n">MPI_Comm</span> <span class="o">*</span><span class="n">comm</span><span class="p">){</span>
    <span class="n">R</span> <span class="n">r</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
    <span class="n">MPI_Allreduce</span><span class="p">(</span><span class="o">&amp;</span><span class="n">s</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">r</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">MPI_TYPE</span><span class="o">&lt;</span><span class="n">R</span><span class="o">&gt;::</span><span class="n">TYPE</span><span class="p">(),</span> <span class="n">MPI_SUM</span><span class="p">,</span> <span class="o">*</span><span class="n">comm</span> <span class="p">);</span>
    <span class="k">return</span> <span class="n">r</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>

<p>This is done in <code class="codehilite">MPIGC</code> dynamics library tool.</p>
<p><strong> Second GMRES algorithm:</strong> Use scharwz algorithm as a preconditioner of basic GMRES method to solving the parallel problem.</p>
<div class="codehilite"><pre><span></span>func real[int] DJ (real[int]&amp; U){ //the original problem
    ++kiter;
    real[int] V(U.n);
    V = Ai*U;
    V = onGi ? 0.: V; //remove boundary term
    return V;
}

func real[int] PDJ (real[int]&amp; U){ //the preconditioner
    real[int] V(U.n);
    real[int] b = onG ? 0. : U;
    V = Ai^-1*b;
    Update(V, U);
    return U;
}
</pre></div>

<p><strong> Third GMRES algorithm:</strong> Add a coarse solver to the previous algorithm</p>
<p>First build a coarse grid on processor 0, and the</p>
<div class="codehilite"><pre><span></span>matrix AC, Rci, Pci;
if (mpiRank(comm) == 0)
    AC = vPbC(VhC, VhC, solver=sparsesolver); //the coarse problem

Pci = interpolate(Whi, VhC); //the projection on coarse grid
Rci = Pci&#39;*Pii; //the restriction on Process i grid with the partition pi_i

func bool CoarseSolve (real[int]&amp; V, real[int]&amp; U, mpiComm&amp; comm){
    // solving the coarse problem
    real[int] Uc(Rci.n), Bc(Uc.n);
    Uc = Rci*U;
    mpiReduce(Uc, Bc, processor(0, comm), mpiSUM);
    if (mpiRank(comm) == 0)
    Uc = AC^-1*Bc;
    broadcast(processor(0, comm), Uc);
    V = Pci*Uc;
}
</pre></div>

<p>The New preconditionner</p>
<div class="codehilite"><pre><span></span>func real[int] PDJC (real[int]&amp; U){
    // Idea: F. Nataf.
    // 0 ~ (I C1A)(I-C2A) =&gt; I ~ - C1AC2A +C1A +C2A
    // New Prec P= C1+C2 - C1AC2 = C1(I- A C2) +C2
    // ( C1(I- A C2) +C2 ) Uo
    // V = - C2*Uo
    // ....
    real[int] V(U.n);
    CoarseSolve(V, U, comm);
    V = -V; //-C2*Uo
    U += Ai*V; //U = (I-A C2) Uo
    real[int] b = onG ? 0. : U;
    U = Ai^-1*b; //C1( I -A C2) Uo
    V = U - V;
    Update(V, U);
    return U;
}
</pre></div>

<p>The code of the 4 algorithms:</p>
<div class="codehilite"><pre><span></span>real epss = 1e-6;
int rgmres = 0;
if (gmres == 1){
    rgmres = MPIAffineGMRES(DJ0, u[], veps=epss, nbiter=300,
        comm=comm, dimKrylov=100, verbosity=ipart?0: 50);
    real[int] b = onG .* u[];
    b = onG ? b : Bi ;
    v[] = Ai^-1*b;
    Update(v[], u[]);
}
else if (gmres == 2)
    rgmres = MPILinearGMRES(DJ, precon=PDJ, u[], Bi, veps=epss,
        nbiter=300, comm=comm, dimKrylov=100, verbosity=ipart?0: 50);
else if (gmres == 3)
    rgmres = MPILinearGMRES(DJ, precon=PDJC, u[], Bi, veps=epss,
        nbiter=300, comm=comm, dimKrylov=100, verbosity=ipart?0: 50);
else //algo Shwarz for demo
    for(int iter = 0; iter &lt; 10; ++iter)
        ...
</pre></div>

<p>We have all ingredient to solve in parallel if we have et the partitions of the unity. To build this partition we do:</p>
<p>The initial step on process <span><span class="MathJax_Preview">1</span><script type="math/tex">1</script></span> to build a coarse mesh, <span><span class="MathJax_Preview">{\mathcal{T}_h}^*</span><script type="math/tex">{\mathcal{T}_h}^*</script></span> of the full domain, and build the partition <span><span class="MathJax_Preview">\pi</span><script type="math/tex">\pi</script></span> function constant equal to <span><span class="MathJax_Preview">i</span><script type="math/tex">i</script></span> on each sub domain <span><span class="MathJax_Preview">\mathcal{O}_i, i =1 ,.., N_p</span><script type="math/tex">\mathcal{O}_i, i =1 ,.., N_p</script></span>, of the grid with the <code class="codehilite">metis</code> graph partitioner <a href="#KARYPIS1995">KARYPIS1995</a> and on each process <span><span class="MathJax_Preview">i</span><script type="math/tex">i</script></span> in <span><span class="MathJax_Preview">1..,N_p</span><script type="math/tex">1..,N_p</script></span> do</p>
<!--- ** --->

<ol>
<li>
<p>Broadcast from process <span><span class="MathJax_Preview">1</span><script type="math/tex">1</script></span>, the mesh <span><span class="MathJax_Preview">{\mathcal{T}_h}^*</span><script type="math/tex">{\mathcal{T}_h}^*</script></span> (call <code class="codehilite">Thii</code> in FreeFem++ script), and <span><span class="MathJax_Preview">\pi</span><script type="math/tex">\pi</script></span> function,
<!--- *** ---></p>
</li>
<li>
<p>remark that the characteristic function <span><span class="MathJax_Preview">\mathrm{1\!\!I}_{\mathcal{O}_i}</span><script type="math/tex">\mathrm{1\!\!I}_{\mathcal{O}_i}</script></span> of domain <span><span class="MathJax_Preview">\mathcal{O}_i</span><script type="math/tex">\mathcal{O}_i</script></span>, is defined by <span><span class="MathJax_Preview">(\pi=i)?1:0</span><script type="math/tex">(\pi=i)?1:0</script></span>,</p>
</li>
<li>
<p>Let us call <span><span class="MathJax_Preview">\Pi^2_P</span><script type="math/tex">\Pi^2_P</script></span> (resp. <span><span class="MathJax_Preview">\Pi^2_V</span><script type="math/tex">\Pi^2_V</script></span>) the <span><span class="MathJax_Preview">L^2</span><script type="math/tex">L^2</script></span> on <span><span class="MathJax_Preview">P_h^*</span><script type="math/tex">P_h^*</script></span> the space of the constant finite element function per element on <span><span class="MathJax_Preview">{\mathcal{T}_h}^*</span><script type="math/tex">{\mathcal{T}_h}^*</script></span> (resp. <span><span class="MathJax_Preview">V_h^*</span><script type="math/tex">V_h^*</script></span> the space of the affine continuous finite element per element on <span><span class="MathJax_Preview">{\mathcal{T}_h}^*</span><script type="math/tex">{\mathcal{T}_h}^*</script></span>) and build in parallel the <span><span class="MathJax_Preview">\pi_i</span><script type="math/tex">\pi_i</script></span> and <span><span class="MathJax_Preview">\Omega_i</span><script type="math/tex">\Omega_i</script></span>, such that <span><span class="MathJax_Preview">\mathcal{O}_i\ \subset \Omega_i</span><script type="math/tex">\mathcal{O}_i\ \subset \Omega_i</script></span> where <span><span class="MathJax_Preview">\mathcal{O}_i= supp ((\Pi^2_V \Pi^2_C)^m \mathrm{1\!\!I}_{O_i})</span><script type="math/tex">\mathcal{O}_i= supp ((\Pi^2_V \Pi^2_C)^m \mathrm{1\!\!I}_{O_i})</script></span>, and <span><span class="MathJax_Preview">m</span><script type="math/tex">m</script></span> is a the overlaps size on the coarse mesh (generally one), (this is done in function <code class="codehilite">AddLayers(Thii,suppii[],nlayer,phii[]);</code> We choose a function <span><span class="MathJax_Preview">\pi^*_i = (\Pi^2_1 \Pi^2_0)^m \mathrm{1\!\!I}_{\mathcal{O}_i}</span><script type="math/tex">\pi^*_i = (\Pi^2_1 \Pi^2_0)^m \mathrm{1\!\!I}_{\mathcal{O}_i}</script></span> so the partition of the unity is simply defined by</p>
<!--- ** --->

<div>
<div class="MathJax_Preview">\begin{equation}
\pi_i = \frac{\pi_i^*}{\sum_{j=1}^{N_p} \pi_j^*}
\end{equation}</div>
<script type="math/tex; mode=display">\begin{equation}
\pi_i = \frac{\pi_i^*}{\sum_{j=1}^{N_p} \pi_j^*}
\end{equation}</script>
</div>
<p>The set <span><span class="MathJax_Preview">J_i</span><script type="math/tex">J_i</script></span> of neighborhood of the domain <span><span class="MathJax_Preview">\Omega_i</span><script type="math/tex">\Omega_i</script></span>, and the local version on <span><span class="MathJax_Preview">V_{hi}</span><script type="math/tex">V_{hi}</script></span> can be defined the array <code class="codehilite">jpart</code> and <code class="codehilite">njpart</code> with:</p>
<div class="codehilite"><pre><span></span>Vhi pii = piistar;
Vhi[int] pij(npij); //local partition of 1 = pii + sum_j pij[j]
int[int] jpart(npart);
int njpart = 0;
Vhi sumphi = piistar;
for (int i = 0; i &lt; npart; ++i)
    if (i != ipart){
        if (int3d(Thi)(pijstar,j) &gt; 0){
            pij[njpart] = pijstar;
            sumphi[] += pij[njpart][];
            jpart[njpart++] = i;
        }
    }
pii[] = pii[] ./ sumphi[];
for (int j = 0; j &lt; njpart; ++j)
    pij[j][] = pij[j][] ./ sumphi[];
jpart.resize(njpart);
</pre></div>

</li>
<li>
<p>We call <span><span class="MathJax_Preview">{\mathcal{T}_h}^*_{ij}</span><script type="math/tex">{\mathcal{T}_h}^*_{ij}</script></span> the sub mesh part of <span><span class="MathJax_Preview">{\mathcal{T}_h}_i</span><script type="math/tex">{\mathcal{T}_h}_i</script></span> where <span><span class="MathJax_Preview">\pi_j</span><script type="math/tex">\pi_j</script></span> are none zero. and thanks to the function <code class="codehilite">trunc</code> to build this array,
    <!--- ** ---></p>
<div class="codehilite"><pre><span></span>for(int jp = 0; jp &lt; njpart; ++jp)
    aThij[jp] = trunc(Thi, pij[jp] &gt; 1e-10, label=10);
</pre></div>

</li>
<li>
<p>At this step we have all on the coarse mesh, so we can build the fine final mesh by splitting all meshes : <code class="codehilite">Thi, Thij[j], Thij[j]</code> with FreeFem++ <code class="codehilite">trunc</code> mesh function which do restriction and slipping.</p>
</li>
<li>
<p>The construction of the send/recv matrices <code class="codehilite">sMj</code> and <code>:::freefemrMj</code>: can done with this code:</p>
<div class="codehilite"><pre><span></span>mesh3 Thij = Thi;
fespace Whij(Thij, Pk);
matrix Pii; Whi wpii = pii; Pii = wpii[]; //Diagonal matrix corresponding X pi_i
matrix[int] sMj(njpart), rMj(njpart); //M send/recive case
 for (int jp = 0; jp &lt; njpart; ++jp){
     int j = jpart[jp];
    Thij = aThij[jp]; //change mesh to change Whij, Whij
    matrix I = interpolate(Whij, Whi); //Whij &lt;- Whi
    sMj[jp] = I*Pii; //Whi -&gt; s Whij
    rMj[jp] = interpolate(Whij, Whi, t=1); //Whij -&gt; Whi
}
</pre></div>

</li>
</ol>
<p>To buil a not too bad application, all variables come from parameters value with the following code</p>
<div class="codehilite"><pre><span></span>include &quot;getARGV.idp&quot;
verbosity = getARGV(&quot;-vv&quot;, 0);
int vdebug = getARGV(&quot;-d&quot;, 1);
int ksplit = getARGV(&quot;-k&quot;, 10);
int nloc = getARGV(&quot;-n&quot;, 25);
string sff = getARGV(&quot;-p, &quot;, &quot;&quot;);
int gmres = getARGV(&quot;-gmres&quot;, 3);
bool dplot = getARGV(&quot;-dp&quot;, 0);
int nC = getARGV(&quot;-N&quot;, max(nloc/10, 4));
</pre></div>

<p>And small include to make graphic in parallel of distribute solution of vector <span><span class="MathJax_Preview">u</span><script type="math/tex">u</script></span> on mesh <span><span class="MathJax_Preview">T_h</span><script type="math/tex">T_h</script></span> with the following interface:</p>
<div class="codehilite"><pre><span></span>include &quot;MPIplot.idp&quot;
func bool plotMPIall(mesh &amp;Th, real[int] &amp;u, string cm){
    PLOTMPIALL(mesh, Pk, Th, u, {cmm=cm, nbiso=20, fill=1, dim=3, value=1});
    return 1;
}
</pre></div>

<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The <code class="codehilite">cmm=cm, ...</code> in the macro argument is a way to quote macro argument so the argument is <code class="codehilite">cmm=cm, ...</code>.</p>
</div>
<!--- The part upper needs to be reviewed --->

<!--- The parts lower have been reviewed --->

<h2 id="parallel-sparse-solvers">Parallel sparse solvers<a class="headerlink" href="#parallel-sparse-solvers" title="Permanent link">#</a></h2>
<p>Parallel sparse solvers use several processors to solve linear systems of equation. Like sequential, parallel linear solvers can be direct or iterative. In <strong><code>FreeFem++</code></strong> both are available.</p>
<h3 id="using-parallel-sparse-solvers-in-freefem">Using parallel sparse solvers in <strong><code>FreeFem++</code></strong><a class="headerlink" href="#using-parallel-sparse-solvers-in-freefem" title="Permanent link">#</a></h3>
<p>We recall that the <code class="codehilite">solver</code> parameters are defined in the following commands: <code class="codehilite">solve</code>, <code class="codehilite">problem</code>, <code class="codehilite">set</code> (setting parameter of a matrix) and in the construction of the matrix corresponding to a bilinear form. In these commands, the parameter <code class="codehilite">solver</code> must be set to <code class="codehilite">sparsesolver</code> for parallel sparse solver. We have added specify parameters to these command lines for parallel sparse solvers. These are</p>
<ul>
<li><code class="codehilite">lparams</code> : vector of integer parameters (<code>l</code> is for the <code>C++</code> type <code class="codehilite"><span class="kt">long</span></code>)</li>
<li><code class="codehilite">dparams</code> : vector of real parameters</li>
<li><code class="codehilite">sparams</code> : string parameters</li>
<li><code class="codehilite">datafilename</code> : name of the file which contains solver parameters</li>
</ul>
<p>The following four parameters are only for direct solvers and are vectors. These parameters allow the user to preprocess the matrix (see the section on <a href="#sparse-direct-solver">sparse direct solver</a> for more information).</p>
<ul>
<li><code class="codehilite">permr</code> : row permutation (integer vector)</li>
<li><code class="codehilite">permc</code> : column permutation or inverse row permutation (integer vector)</li>
<li><code class="codehilite">scaler</code> : row scaling (real vector)</li>
<li><code class="codehilite">scalec</code> : column scaling (real vector)</li>
</ul>
<p>There are two possibilities to control solver parameters. The first method defines parameters with <code class="codehilite">lparams</code>, <code class="codehilite">dparams</code> and <code class="codehilite">sparams</code> in <code>.edp</code> file.</p>
<p>The second one reads the solver parameters from a data file. The name of this file is specified by <code class="codehilite">datafilename</code>. If <code class="codehilite">lparams</code>, <code class="codehilite">dparams</code>, <code class="codehilite">sparams</code> or <code class="codehilite">datafilename</code> is not provided by the user, the solver's default values are used.</p>
<p>To use parallel solver in <strong><code>FreeFem++</code></strong>, we need to load the dynamic library corresponding to this solver. For example to use <a href="http://mumps.enseeiht.fr/">MUMPS</a> solver as parallel solver in <strong><code>FreeFem++</code></strong>, write in the <code>.edp</code> file <code class="codehilite">load &quot;MUMPS_FreeFem&quot;</code>.</p>
<p>If the libraries are not loaded, the default sparse solver will be loaded (default sparse solver is <code class="codehilite">UMFPACK</code>). The <a href="#Tab1">table 1</a> gives this new value for the different libraries.</p>
<table>
    <thead>
        <tr>
            <th colspan="3"><a name="Tab1">Table 1</a>: Default sparse solver for real and complex arithmetics when we load a parallel sparse solver library</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td rowspan="2" style="vertical-align: middle !important; font-weight: bold">Libraries</td>
            <td colspan="2" align="center" style="font-weight: bold">Default sparse solver</td>
        </tr>
        <tr>
            <td align="center" style="font-weight: bold">real</td>
            <td align="center" style="font-weight: bold">complex</td>
        </tr>
        <tr>
            <td>MUMPS_FreeFem</td>
            <td align="center">mumps</td>
            <td align="center">mumps</td>
        </tr>
        <tr>
            <td>real_SuperLU_DIST_FreeFem</td>
            <td align="center">SuperLU_DIST</td>
            <td align="center">previous solver</td>
        </tr>
        <tr>
            <td>complex_SuperLU_DIST_FreeFem</td>
            <td align="center">previous solver</td>
            <td align="center">SuperLU_DIST</td>
        </tr>
        <tr>
            <td>real_pastix_FreeFem</td>
            <td align="center">PaStiX</td>
            <td align="center">previous solver</td>
        </tr>
        <tr>
            <td>complex_pastix_FreeFem</td>
            <td align="center">previous solver</td>
            <td align="center">PaStiX</td>
        </tr>
        <tr>
            <td>hips_FreeFem</td>
            <td align="center">hips</td>
            <td align="center">previous solver</td>
        </tr>
        <tr>
            <td>hypre_FreeFem</td>
            <td align="center">hypre</td>
            <td align="center">previous solver</td>
        </tr>
        <tr>
            <td>parms_FreeFem</td>
            <td align="center">parms</td>
            <td align="center">previous solver</td>
        </tr>
    </tbody>
</table>

<p>We also add functions (see <a href="#Tab2">Table 2</a>) with no parameter to change the default sparse solver in the <code>.edp</code> file. To use these functions, we need to load the library corresponding to the solver. An example of using different parallel sparse solvers for the same problem is given in <a href="../examples/#direct-solvers">Direct solvers example</a>.</p>
<table>
    <thead>
        <tr>
            <th colspan="3"><a name="Tab2">Table 2</a>: Functions that allow to change the default sparse solver for real and complex arithmetics and the result of these functions</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td rowspan="2" style="vertical-align: bottom !important; font-weight: bold">Function</td>
            <td colspan="2" align="center" style="font-weight: bold">default sparse solver</td>
        </tr>
        <tr>
            <td align="center" style="font-weight: bold">real</td>
            <td align="center" style="font-weight: bold">complex</td>
        </tr>
        <tr>
            <td>defaulttoMUMPS()</td>
            <td align="center">mumps</td>
            <td align="center">mumps</td>
        </tr>
        <tr>
            <td>realdefaulttoSuperLUdist()</td>
            <td align="center">SuperLU_DIST</td>
            <td align="center">previous solver</td>
        </tr>
        <tr>
            <td>complexdefaulttoSuperLUdist()</td>
            <td align="center">previous solver</td>
            <td align="center">SuperLU_DIST</td>
        </tr>
        <tr>
            <td>realdefaultopastix()</td>
            <td align="center">pastix</td>
            <td align="center">previous solver</td>
        </tr>
        <tr>
            <td>complexdefaulttopastix()</td>
            <td align="center">previous solver</td>
            <td align="center">pastix</td>
        </tr>
        <tr>
            <td>defaulttohips()</td>
            <td align="center">hips</td>
            <td align="center">previous solver</td>
        </tr>
        <tr>
            <td>defaulttohypre()</td>
            <td align="center">hypre</td>
            <td align="center">previous solver</td>
        </tr>
        <tr>
            <td>defaulttoparms()</td>
            <td align="center">parms</td>
            <td align="center">previous solver</td>
        </tr>
    </tbody>
</table>

<div class="admonition example">
<p class="admonition-title">Test direct solvers</p>
<div class="codehilite"><pre><span></span>load &quot;MUMPS_FreeFem&quot;
//default solver: real-&gt; MUMPS, complex -&gt; MUMPS
load &quot;real_SuperLU_DIST_FreeFem&quot;
//default solver: real-&gt; SuperLU_DIST, complex -&gt; MUMPS
load &quot;real_pastix_FreeFem&quot;
//default solver: real-&gt; pastix, complex -&gt; MUMPS

// Solving with pastix
{
    matrix A =
        [[1, 2, 2, 1, 1],
        [ 2, 12, 0, 10 , 10],
        [ 2, 0, 1, 0, 2],
        [ 1, 10, 0, 22, 0.],
        [ 1, 10, 2, 0., 22]];

    real[int] xx = [1, 32, 45, 7, 2], x(5), b(5), di(5);
    b = A*xx;
    cout &lt;&lt; &quot;b = &quot; &lt;&lt; b &lt;&lt; endl;
    cout &lt;&lt; &quot;xx = &quot; &lt;&lt; xx &lt;&lt; endl;

    set(A, solver=sparsesolver, datafilename=&quot;ffpastix_iparm_dparm.txt&quot;);
    cout &lt;&lt; &quot;solve&quot; &lt;&lt; endl;
    x = A^-1*b;
    cout &lt;&lt; &quot;b = &quot; &lt;&lt; b &lt;&lt; endl;
    cout &lt;&lt; &quot;x = &quot; &lt;&lt; endl;
    cout &lt;&lt; x &lt;&lt; endl;
    di = xx - x;
    if (mpirank == 0){
        cout &lt;&lt; &quot;x-xx = &quot; &lt;&lt; endl;
        cout &lt;&lt; &quot;Linf = &quot; &lt;&lt; di.linfty &lt;&lt; &quot;, L2 = &quot; &lt;&lt; di.l2 &lt;&lt; endl;
    }
}

// Solving with SuperLU_DIST
realdefaulttoSuperLUdist();
//default solver: real-&gt; SuperLU_DIST, complex -&gt; MUMPS
{
    matrix A =
        [[1, 2, 2, 1, 1],
        [ 2, 12, 0, 10 , 10],
        [ 2, 0, 1, 0, 2],
        [ 1, 10, 0, 22, 0.],
        [ 1, 10, 2, 0., 22]];

    real[int] xx = [1, 32, 45, 7, 2], x(5), b(5), di(5);
    b = A*xx;
    cout &lt;&lt; &quot;b = &quot; &lt;&lt; b &lt;&lt; endl;
    cout &lt;&lt; &quot;xx = &quot; &lt;&lt; xx &lt;&lt; endl;

    set(A, solver=sparsesolver, datafilename=&quot;ffsuperlu_dist_fileparam.txt&quot;);
    cout &lt;&lt; &quot;solve&quot; &lt;&lt; endl;
    x = A^-1*b;
    cout &lt;&lt; &quot;b = &quot; &lt;&lt; b &lt;&lt; endl;
    cout &lt;&lt; &quot;x = &quot; &lt;&lt; endl;
    cout &lt;&lt; x &lt;&lt; endl;
    di = xx - x;
    if (mpirank == 0){
        cout &lt;&lt; &quot;x-xx = &quot; &lt;&lt; endl;
        cout &lt;&lt; &quot;Linf = &quot; &lt;&lt; di.linfty &lt;&lt; &quot;, L2 = &quot; &lt;&lt; di.l2 &lt;&lt; endl;
    }
}

// Solving with MUMPS
defaulttoMUMPS();
//default solver: real-&gt; MUMPS, complex -&gt; MUMPS
{
    matrix A =
        [[1, 2, 2, 1, 1],
        [ 2, 12, 0, 10 , 10],
        [ 2, 0, 1, 0, 2],
        [ 1, 10, 0, 22, 0.],
        [ 1, 10, 2, 0., 22]];

    real[int] xx = [1, 32, 45, 7, 2], x(5), b(5), di(5);
    b = A*xx;
    cout &lt;&lt; &quot;b = &quot; &lt;&lt; b &lt;&lt; endl;
    cout &lt;&lt; &quot;xx = &quot; &lt;&lt; xx &lt;&lt; endl;

    set(A, solver=sparsesolver, datafilename=&quot;ffmumps_fileparam.txt&quot;);
    cout &lt;&lt; &quot;solving solution&quot; &lt;&lt; endl;
    x = A^-1*b;
    cout &lt;&lt; &quot;b = &quot; &lt;&lt; b &lt;&lt; endl;
    cout &lt;&lt; &quot;x = &quot; &lt;&lt; endl;
    cout &lt;&lt; x &lt;&lt; endl;
    di = xx - x;
    if (mpirank == 0){
        cout &lt;&lt; &quot;x-xx = &quot; &lt;&lt; endl;
        cout &lt;&lt; &quot;Linf = &quot; &lt;&lt; di.linfty &lt;&lt; &quot;, L2 &quot; &lt;&lt; di.l2 &lt;&lt; endl;
    }
}
</pre></div>

</div>
<h3 id="sparse-direct-solver">Sparse direct solver<a class="headerlink" href="#sparse-direct-solver" title="Permanent link">#</a></h3>
<p>In this section, we present the sparse direct solvers interfaced with <strong><code>FreeFem++</code></strong>.</p>
<h4 id="mumps-solver">MUMPS solver<a class="headerlink" href="#mumps-solver" title="Permanent link">#</a></h4>
<p>MUltifrontal Massively Parallel Solver (<a href="http://mumps.enseeiht.fr/">MUMPS</a>) is an open-source library.</p>
<p>This package solves linear system of the form <span><span class="MathJax_Preview">A \: x = b</span><script type="math/tex">A \: x = b</script></span> where <span><span class="MathJax_Preview">A</span><script type="math/tex">A</script></span> is a square sparse matrix with a direct method. The square matrix considered in MUMPS can be either unsymmetric, symmetric positive definite or general symmetric.</p>
<p>The method implemented in MUMPS is a direct method based on a multifrontal approach. It constructs a direct factorization <span><span class="MathJax_Preview">A \:= \: L\:U</span><script type="math/tex">A \:= \: L\:U</script></span>, <span><span class="MathJax_Preview">A\: = \: L^t \: D \: L</span><script type="math/tex">A\: = \: L^t \: D \: L</script></span> depending of the symmetry of the matrix <span><span class="MathJax_Preview">A</span><script type="math/tex">A</script></span>.</p>
<p>MUMPS uses the following libraries : <a href="http://www.netlib.org/blas/">BLAS</a>, <a href="http://www.netlib.org/blacs/">BLACS</a> and <a href="http://www.netlib.org/scalapack/">ScaLAPACK</a>.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>MUMPS does not solve linear system with a rectangular matrix.</p>
</div>
<p><strong>MUMPS parameters:</strong></p>
<p>There are four input parameters in <a href="http://mumps.enseeiht.fr/index.php?page=doc">MUMPS</a>. Two integers <code class="codehilite"><span class="n">SYM</span></code> and <code class="codehilite"><span class="n">PAR</span></code>, a vector of integer of size 40 <code class="codehilite"><span class="n">INCTL</span></code> and a vector of real of size 15 <code class="codehilite"><span class="n">CNTL</span></code>.</p>
<p>The first parameter gives the type of the matrix: 0 for unsymmetric matrix, 1 for symmetric positive matrix and 2 for general symmetric.</p>
<p>The second parameter defined if the host processor work during the factorization and solves steps : <code class="codehilite"><span class="n">PAR</span><span class="o">=</span><span class="mi">1</span></code> host processor working and <code class="codehilite"><span class="n">PAR</span><span class="o">=</span><span class="mi">0</span></code> host processor not working.</p>
<p>The parameter <code class="codehilite"><span class="n">INCTL</span></code> and <code class="codehilite"><span class="n">CNTL</span></code> is the control parameter of MUMPS. The vectors <code class="codehilite"><span class="n">ICNTL</span></code> and <code class="codehilite"><span class="n">CNTL</span></code> in MUMPS becomes with index 1 like vector in <code>Fortran</code>. For more details see the <a href="http://mumps.enseeiht.fr/index.php?page=doc">MUMPS user's guide</a>.</p>
<p>We describe now some elements of the main parameters of <code class="codehilite"><span class="n">ICNTL</span></code> for MUMPS.</p>
<ul>
<li>
<p><strong>Input matrix parameter</strong>
    The input matrix is controlled by parameters <code>ICNTL(5)</code> and <code>ICNTL(18)</code>. The matrix format (resp. matrix pattern and matrix entries) are controlled by <code>INCTL(5)</code> (resp. <code>INCTL(18)</code>).</p>
<p>The different values of <code>ICNTL(5)</code> are 0 for assembled format and 1 for element format. In the current release of <strong><code>FreeFem++</code></strong>, we consider that FE matrix or matrix is storage in assembled format. Therefore, <code>INCTL(5)</code> is treated as 0 value.</p>
<p>The main option for <code>ICNTL(18)</code>: <code>INCLTL(18)=0</code> centrally on the host processor, <code>ICNTL(18)=3</code> distributed the input matrix pattern and the entries (recommended option for distributed matrix by developer of MUMPS). For other values of <code>ICNTL(18)</code> see the <a href="http://mumps.enseeiht.fr/index.php?page=doc">MUMPS user's guide</a>. These values can be used also in <strong><code>FreeFem++</code></strong>.</p>
<p>The default option implemented in <strong><code>FreeFem++</code></strong> are <code>ICNTL(5)=0</code> and <code>ICNTL(18)=0</code>.</p>
</li>
<li>
<p><strong>Preprocessing parameter</strong>
    The preprocessed matrix <span><span class="MathJax_Preview">A_{p}</span><script type="math/tex">A_{p}</script></span> that will be effectively factored is defined by
    <script type="math/tex; mode=display">
    A_{p} = P \: D_r \: A \: Q_c \ D_c P^t
    </script>
    where <span><span class="MathJax_Preview">P</span><script type="math/tex">P</script></span> is the permutation matrix, <span><span class="MathJax_Preview">Q_c</span><script type="math/tex">Q_c</script></span> is the column permutation, <span><span class="MathJax_Preview">D_r</span><script type="math/tex">D_r</script></span> and <span><span class="MathJax_Preview">D_c</span><script type="math/tex">D_c</script></span> are diagonal matrix for respectively row and column scaling.</p>
<p>The ordering strategy to obtain <span><span class="MathJax_Preview">P</span><script type="math/tex">P</script></span> is controlled by parameter <code>ICNTL(7)</code>. The permutation of zero free diagonal <span><span class="MathJax_Preview">Q_c</span><script type="math/tex">Q_c</script></span> is controlled by parameter <code>ICNTL(6)</code>. The row and column scaling is controlled by parameter <code>ICNTL(18)</code>. These option are connected and also strongly related with <code>ICNTL(12)</code> (see the <a href="http://mumps.enseeiht.fr/index.php?page=doc">MUMPS user's guide</a> for more details).</p>
<p>The parameters <code class="codehilite">permr</code>, <code class="codehilite">scaler</code>, and <code class="codehilite">scalec</code> in <strong><code>FreeFem++</code></strong> allow to give permutation matrix(<span><span class="MathJax_Preview">P</span><script type="math/tex">P</script></span>), row scaling (<span><span class="MathJax_Preview">D_r</span><script type="math/tex">D_r</script></span>) and column scaling (<span><span class="MathJax_Preview">D_c</span><script type="math/tex">D_c</script></span>) of the user respectively.</p>
</li>
</ul>
<p><strong>Calling MUMPS in <code>FreeFem++</code></strong></p>
<p>To call MUMPS in <strong><code>FreeFem++</code></strong>, we need to load the dynamic library <code>MUMPS_freefem.dylib</code> (MacOSX), <code>MUMPS_freefem.so</code> (Unix) or <code>MUMPS_freefem.dll</code> (Windows).</p>
<p>This is done in typing <code class="codehilite">load &quot;MUMPS_FreeFem&quot;</code> in the <code>.edp</code> file. We give now the two methods to give the option of MUMPS solver in <strong><code>FreeFem++</code></strong>.</p>
<ul>
<li>
<p><strong>Solver parameters is defined in .edp file:</strong>
    In this method, we need to give the parameters <code class="codehilite">lparams</code> and <code class="codehilite">dparams</code>. These parameters are defined for MUMPS by :</p>
<p><code class="codehilite">lparams[0] = SYM</code>,<br>
<code class="codehilite">lparams[1] = PAR</code>,<br>
<span><span class="MathJax_Preview">\forall i</span><script type="math/tex">\forall i</script></span> = 1,...,40, <code class="codehilite">lparams[i+1] = ICNTL(i)</code><br>
<span><span class="MathJax_Preview">\forall i</span><script type="math/tex">\forall i</script></span> = 1,...,15, <code class="codehilite">dparams[i-1] = CNTL(i)</code></p>
</li>
<li>
<p><strong>Reading solver parameters on a file:</strong></p>
<p>The structure of data file for MUMPS in <strong><code>FreeFem++</code></strong> is : first line parameter <code>SYM</code> and second line parameter <code>PAR</code> and in the following line the different value of vectors <code>ICNTL</code> and <code>CNTL</code>. An example of this parameter file is given in <code class="codehilite">ffmumpsfileparam.txt</code>.</p>
<div class="codehilite"><pre><span></span>0 /* SYM :: 0 for non symmetric matrix, 1 for symmetric definite positive matrix and 2 general symmetric matrix*/
1 /* PAR :: 0 host not working during factorization and solves steps, 1 host working during factorization and solves steps*/
-1 /* ICNTL(1) :: output stream for error message */
-1 /* ICNTL(2) :: output for diagnostic printing, statics and warning message */
-1 /* ICNTL(3) :: for global information */
0 /* ICNTL(4) :: Level of printing for error, warning and diagnostic message */
0 /* ICNTL(5) :: matrix format : 0 assembled format, 1 elemental format. */
7 /* ICNTL(6) :: control option for permuting and/or scaling the matrix in analysis phase */
3 /* ICNTL(7) :: pivot order strategy : AMD, AMF, metis, pord scotch*/
77 /* ICNTL(8) :: Row and Column scaling strategy */
1 /* ICNTL(9) :: 0 solve Ax = b, 1 solve the transposed system A^t x = b : parameter is not considered in the current release of freefem++*/
0 /* ICNTL(10) :: number of steps of iterative refinement */
0 /* ICNTL(11) :: statics related to linear system depending on ICNTL(9) */
1 /* ICNTL(12) :: constrained ordering strategy for general symmetric matrix */
0 /* ICNTL(13) :: method to control splitting of the root frontal matrix */
20 /* ICNTL(14) :: percentage increase in the estimated working space (default 20\%)*/
0 /* ICNTL(15) :: not used in this release of MUMPS */
0 /* ICNTL(16) :: not used in this release of MUMPS */
0 /* ICNTL(17) :: not used in this release of MUMPS */
3 /* ICNTL(18) :: method for given : matrix pattern and matrix entries : */
0 /* ICNTL(19) :: method to return the Schur complement matrix */
0 /* ICNTL(20) :: right hand side form ( 0 dense form, 1 sparse form) : parameter will be set to 0 for FreeFem++ */
0 /* ICNTL(21) :: 0, 1 kept distributed solution : parameter is not considered in the current release of FreeFem++ */
0 /* ICNTL(22) :: controls the in-core/out-of-core (OOC) facility */
0 /* ICNTL(23) :: maximum size of the working memory in Megabyte than MUMPS can allocate per working processor */
0 /* ICNTL(24) :: control the detection of null pivot */
0 /* ICNTL(25) :: control the computation of a null space basis */
0 /* ICNTL(26) :: This parameter is only significant with Schur option (ICNTL(19) not zero). : parameter is not considered in the current release of FreeFem++ */
-8 /* ICNTL(27) (Experimental parameter subject to change in next release of MUMPS) :: control the blocking factor for multiple righthand side during the solution phase : parameter is not considered in the current release of FreeFem++ */
0 /* ICNTL(28) :: not used in this release of MUMPS*/
0 /* ICNTL(29) :: not used in this release of MUMPS*/
0 /* ICNTL(30) :: not used in this release of MUMPS*/
0 /* ICNTL(31) :: not used in this release of MUMPS*/
0 /* ICNTL(32) :: not used in this release of MUMPS*/
0 /* ICNTL(33) :: not used in this release of MUMPS*/
0 /* ICNTL(34) :: not used in this release of MUMPS*/
0 /* ICNTL(35) :: not used in this release of MUMPS*/
0 /* ICNTL(36) :: not used in this release of MUMPS*/
0 /* ICNTL(37) :: not used in this release of MUMPS*/
0 /* ICNTL(38) :: not used in this release of MUMPS*/
1 /* ICNTL(39) :: not used in this release of MUMPS*/
0 /* ICNTL(40) :: not used in this release of MUMPS*/
0.01 /* CNTL(1) :: relative threshold for numerical pivoting */
1e-8 /* CNTL(2) :: stopping criteria for iterative refinement */
-1 /* CNTL(3) :: threshold for null pivot detection */
-1 /* CNTL(4) :: determine the threshold for partial pivoting */
0.0 /* CNTL(5) :: fixation for null pivots */
0 /* CNTL(6) :: not used in this release of MUMPS */
0 /* CNTL(7) :: not used in this release of MUMPS */
0 /* CNTL(8) :: not used in this release of MUMPS */
0 /* CNTL(9) :: not used in this release of MUMPS */
0 /* CNTL(10) :: not used in this release of MUMPS */
0 /* CNTL(11) :: not used in this release of MUMPS */
0 /* CNTL(12) :: not used in this release of MUMPS */
0 /* CNTL(13) :: not used in this release of MUMPS */
0 /* CNTL(14) :: not used in this release of MUMPS */
0 /* CNTL(15) :: not used in this release of MUMPS */
</pre></div>

</li>
</ul>
<p>If no solver parameter is given, we used default option of MUMPS solver.</p>
<div class="admonition example">
<p class="admonition-title">MUMPS example</p>
<p>A simple example of calling MUMPS in <strong><code>FreeFem++</code></strong> with this two methods is given in the <a href="../examples/#solver-mumps">Test solver MUMPS example</a>.</p>
</div>
<h4 id="superlu-distributed-solver">SuperLU distributed solver<a class="headerlink" href="#superlu-distributed-solver" title="Permanent link">#</a></h4>
<p>The package <a href="http://crd-legacy.lbl.gov/~xiaoye/SuperLU/">SuperLU_DIST</a> solves linear systems using LU factorization. It is a free scientific library under BSD license.</p>
<p>This library provides functions to handle square or rectangular matrix in real and complex arithmetics. The method implemented in SuperLU_DIST is a supernodal method. New release of this package includes a parallel symbolic factorization. This scientific library is written in C and MPI for communications.</p>
<p><strong>SuperLU_DIST parameters:</strong></p>
<p>We describe now some parameters of SuperLU_DIST. The SuperLU_DIST library use a 2D-logical process group. This process grid is specified by <span><span class="MathJax_Preview">nprow</span><script type="math/tex">nprow</script></span> (process row) and <span><span class="MathJax_Preview">npcol</span><script type="math/tex">npcol</script></span> (process column) such that <span><span class="MathJax_Preview">N_{p} = nprow \: npcol</span><script type="math/tex">N_{p} = nprow \: npcol</script></span> where <span><span class="MathJax_Preview">N_{p}</span><script type="math/tex">N_{p}</script></span> is the number of all process allocated for SuperLU_DIST.</p>
<p>The input matrix parameters is controlled by "matrix= " in <code class="codehilite">sparams</code> for internal parameter or in the third line of parameters file. The different value are</p>
<ul>
<li><code class="codehilite">matrix=assembled</code> global matrix are available on all process</li>
<li><code class="codehilite">matrix=distributedglobal</code> The global matrix is distributed among all the process</li>
<li><code class="codehilite">matrix=distributed</code> The input matrix is distributed (not yet implemented)</li>
</ul>
<p>The option arguments of SuperLU_DIST are described in the section Users-callable routine of the <a href="http://crd-legacy.lbl.gov/~xiaoye/SuperLU/ug.pdf">SuperLU users' guide</a>.</p>
<p>The parameter <code>Fact</code> and <code>TRANS</code> are specified in <strong><code>FreeFem++</code></strong> interfaces to SuperLU_DIST during the different steps. For this reason, the value given by the user for this option is not considered.</p>
<p>The factorization LU is calculated in SuperLU_DIST on the matrix <span><span class="MathJax_Preview">A_p</span><script type="math/tex">A_p</script></span>.
<script type="math/tex; mode=display">
A_{p} = P_{c} \: P_r \: D_r \: A \: D_{c} \: P_{c}^{t}
</script>
where <span><span class="MathJax_Preview">P_c</span><script type="math/tex">P_c</script></span> and <span><span class="MathJax_Preview">P_r</span><script type="math/tex">P_r</script></span> is the row and column permutation matrix respectively, <span><span class="MathJax_Preview">D_r</span><script type="math/tex">D_r</script></span> and <span><span class="MathJax_Preview">D_c</span><script type="math/tex">D_c</script></span> are diagonal matrix for respectively row and column scaling.</p>
<p>The option argument <code>RowPerm</code> (resp. <code>ColPerm</code>) control the row (resp. column) permutation matrix. <span><span class="MathJax_Preview">D_r</span><script type="math/tex">D_r</script></span> and <span><span class="MathJax_Preview">D_c</span><script type="math/tex">D_c</script></span> is controlled by the parameter <code>DiagScale</code>.</p>
<p>The parameter <code class="codehilite">permr</code>, <code class="codehilite">permc</code>, <code class="codehilite">scaler</code>, and <code class="codehilite">scalec</code> in <strong><code>FreeFem++</code></strong> is provided to give row permutation, column permutation, row scaling and column scaling of the user respectively.</p>
<p>The other parameters for LU factorization are <code>ParSymFact</code> and <code>ReplaceTinyPivot</code>. The parallel symbolic factorization works only on a power of two processes and need the <code>ParMetis</code> ordering. The default option argument of SuperLU_DIST are given in the file <code>ffsuperlu_dist_fileparam.txt</code>.</p>
<p><strong>Calling SuperLU_DIST in </strong><code>FreeFem++</code>____</p>
<p>To call SuperLU_DIST in <strong><code>FreeFem++</code></strong>, we need to load the library dynamic correspond to interface. This done by the following line <code class="codehilite">load &quot;real_superlu _DIST_FreeFem&quot;</code> (resp. <code class="codehilite">load &quot;complex_superlu_DIST_FreeFem&quot;</code>) for real (resp. complex) arithmetics in the file <code>.edp</code>.</p>
<p><strong>Solver parameters is defined in <code>.edp</code> file:</strong></p>
<p>To call SuperLU_DIST with internal parameter, we used the parameters <code>sparams</code>. The value of parameters of SuperLU_DIST in <code>sparams</code> are defined by :</p>
<ul>
<li><code>nprow=1</code>,</li>
<li><code>npcol=1</code>,</li>
<li><code>matrix= distributedgloba</code>,</li>
<li><code>Fact= DOFACT</code>,</li>
<li><code>Equil=NO</code>,</li>
<li><code>ParSymbFact=NO</code>,</li>
<li><code>ColPerm= MMD_AT_PLUS_A</code>,</li>
<li><code>RowPerm= LargeDiag</code>,</li>
<li><code>DiagPivotThresh=1.0</code>,</li>
<li><code>IterRefine=DOUBLE</code>,</li>
<li><code>Trans=NOTRANS</code>,</li>
<li><code>ReplaceTinyPivot=NO</code>,</li>
<li><code>SolveInitialized=NO</code>,</li>
<li><code>PrintStat=NO</code>,</li>
<li><code>DiagScale=NOEQUIL</code></li>
</ul>
<p>This value correspond to the parameter in the file <code>ffsuperlu_dist_fileparam.txt</code>. If one parameter is not specified by the user, we take the default value of SuperLU_DIST.</p>
<p><strong>Reading solver parameters on a file:</strong>
The structure of data file for SuperLU_DIST in <strong><code>FreeFem++</code></strong> is given in the file <code>ffsuperlu_dist_fileparam.txt</code> (default value of the <strong><code>FreeFem++</code></strong> interface).</p>
<div class="codehilite"><pre><span></span>1 /* nprow : integer value */
1 /* npcol : integer value */
distributedglobal /* matrix input : assembled, distributedglobal, distributed */
DOFACT /* Fact : DOFACT, SamePattern, SamePattern_SameRowPerm, FACTORED */
NO /* Equil : NO, YES */
NO /* ParSymbFact : NO, YES */
MMD_AT_PLUS_A /* ColPerm : NATURAL, MMD_AT_PLUS_A, MMD_ATA, METIS_AT_PLUS_A, PARMETIS, MY_PERMC */
LargeDiag /* RowPerm : NOROWPERM, LargeDiag, MY_PERMR */
1.0 /* DiagPivotThresh : real value */
DOUBLE /* IterRefine : NOREFINE, SINGLE, DOUBLE, EXTRA */
NOTRANS /* Trans : NOTRANS, TRANS, CONJ*/
NO /* ReplaceTinyPivot : NO, YES*/
NO /* SolveInitialized : NO, YES*/
NO /* RefineInitialized : NO, YES*/
NO /* PrintStat : NO, YES*/
NOEQUIL /* DiagScale : NOEQUIL, ROW, COL, BOTH*/
</pre></div>

<p>If no solver parameter is given, we used default option of SuperLU_DIST solver.</p>
<div class="admonition example">
<p class="admonition-title">Example</p>
<p>A simple example of calling SuperLU_DIST in <strong><code>FreeFem++</code></strong> with this two methods is given in the <a href="../examples/#solver-superlu_dist">Solver superLU_DIST example</a>.</p>
</div>
<h4 id="pastix-solver">PaStiX solver<a class="headerlink" href="#pastix-solver" title="Permanent link">#</a></h4>
<p><a href="http://pastix.gforge.inria.fr/files/README-txt.html">PaStiX</a> (Parallel Sparse matrix package) is a free scientific library under CECILL-C license. This package solves sparse linear system with a direct and block ILU(k) iterative methods. This solver can be applied to a real or complex matrix with a symmetric pattern.</p>
<p><strong>PaStiX parameters:</strong></p>
<p>The input <code class="codehilite">matrix</code> parameter of <strong><code>FreeFem++</code></strong> depend on PaStiX interface. <code class="codehilite">matrix = assembled</code> for non distributed matrix. It is the same parameter for SuperLU_DIST.</p>
<p>There are four parameters in PaStiX : <code>iparm</code>, <code>dparm</code>, <code>perm</code> and <code>invp</code>. These parameters are respectively the integer parameters (vector of size 64), real parameters (vector of size 64), permutation matrix and inverse permutation matrix respectively. <code>iparm</code> and <code>dparm</code> vectors are described in <a href="https://gforge.inria.fr/docman/?group_id=186&amp;view=listfile&amp;dirid=246">PaStiX RefCard</a>.</p>
<p>The parameters <code class="codehilite">permr</code> and <code class="codehilite">permc</code> in <strong><code>FreeFem++</code></strong> are provided to give permutation matrix and inverse permutation matrix of the user respectively.</p>
<p><strong>Solver parameters defined in <code>.edp</code> file:</strong></p>
<p>To call PaStiX in <strong><code>FreeFem++</code></strong> in this case, we need to specify the parameters <code class="codehilite">lparams</code> and <code class="codehilite">dparams</code>. These parameters are defined by :</p>
<p><span><span class="MathJax_Preview">\forall i</span><script type="math/tex">\forall i</script></span> = 0,... ,63, <code class="codehilite">lparams[i] = iparm[i]</code>.</p>
<p><span><span class="MathJax_Preview">\forall i</span><script type="math/tex">\forall i</script></span> = 0,... ,63, <code class="codehilite">dparams[i] = dparm[i]</code>.</p>
<p><strong>Reading solver parameters on a file:</strong></p>
<p>The structure of data file for PaStiX parameters in <strong><code>FreeFem++</code></strong> is : first line structure parameters of the matrix and in the following line the value of vectors <code>iparm</code> and <code>dparm</code> in this order.</p>
<div class="codehilite"><pre><span></span>assembled /* matrix input :: assembled, distributed global and distributed */
iparm[0]
iparm[1]
...
...
iparm[63]
dparm[0]
dparm[1]
...
...
dparm[63]
</pre></div>

<p>An example of this file parameter is given in <code>ffpastix_iparm_dparm.txt</code> with a description of these parameters. This file is obtained with the example file <code>iparm.txt</code> and <code>dparm.txt</code> including in the PaStiX package.</p>
<p>If no solver parameter is given, we use the default option of PaStiX solver.</p>
<div class="admonition example">
<p class="admonition-title">Example</p>
<p>A simple example of calling PaStiX in <strong><code>FreeFem++</code></strong> with this two methods is given in the <a href="../examples/#solver-pastix">Solver PaStiX example</a>.</p>
</div>
<p>In <a href="#Tab3">Table 3</a>, we recall the different matrix considering in the different direct solvers.</p>
<table>
    <thead>
        <tr>
            <th colspan="7"><a name="Tab3">Table 3</a>: Type of matrix used by the different direct sparse solver</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td rowspan="2" style="vertical-align: bottom !important; font-weight: bold">direct solver</td>
            <td colspan="3" align="center" style="font-weight: bold">square matrix</td>
            <td colspan="3" align="center" style="font-weight: bold">rectangular matrix</td>
        </tr>
        <tr>
            <td style="font-weight: bold">sym</td>
            <td align="center" style="font-weight: bold">sym pattern</td>
            <td align="center" style="font-weight: bold">unsym</td>
            <td align="center" style="font-weight: bold">sym</td>
            <td align="center" style="font-weight: bold">sym pattern</td>
            <td align="center" style="font-weight: bold">unsym</td>
        </tr>
        <tr>
            <td>SuperLU_DIST</td>
            <td align="center">yes</td>
            <td align="center">yes</td>
            <td align="center">yes</td>
            <td align="center">yes</td>
            <td align="center">yes</td>
            <td align="center">yes</td>
        </tr>
        <tr>
            <td>MUMPS</td>
            <td align="center">yes</td>
            <td align="center">yes</td>
            <td align="center">yes</td>
            <td align="center">no</td>
            <td align="center">no</td>
            <td align="center">no</td>
        </tr>
        <tr>
            <td>pastix</td>
            <td align="center">yes</td>
            <td align="center">yes</td>
            <td align="center">no</td>
            <td align="center">no</td>
            <td align="center">no</td>
            <td align="center">no</td>
        </tr>
    </tbody>
</table>

<h3 id="parallel-sparse-iterative-solver">Parallel sparse iterative solver<a class="headerlink" href="#parallel-sparse-iterative-solver" title="Permanent link">#</a></h3>
<p>Concerning iterative solvers, we have chosen <a href="http://www-users.cs.umn.edu/~saad/software/pARMS/">pARMS</a>, <a href="http://hips.gforge.inria.fr/">HIPS</a> and <a href="https://computation.llnl.gov/projects/hypre-scalable-linear-solvers-multigrid-methods">Hypre</a>.</p>
<p>Each software implements a different type of parallel preconditioner.</p>
<p>So, pARMS implements algebraic domain decomposition preconditioner type such as additive Schwartz <a href="#CAI1989">CAI1989</a> and interface method; while HIPS implement hierarchical incomplete factorization and finally HYPRE implements multilevel preconditioner are AMG(Algebraic MultiGrid) and parallel approximated inverse.</p>
<p>To use one of these programs in <strong><code>FreeFem++</code></strong>, you have to install it independently of <strong><code>FreeFem++</code></strong>. It is also necessary to install the MPI communication library which is essential for communication between the processors and, in some cases, software partitioning graphs like <a href="http://glaros.dtc.umn.edu/gkhome/metis/metis/overview">METIS</a> or <a href="http://www.labri.fr/perso/pelegrin/scotch/">Scotch</a>.</p>
<p>All this preconditioners are used with Krylov subspace methods accelerators.</p>
<p>Krylov subspace methods are iterative methods which consist in finding a solution <span><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span> of linear system <span><span class="MathJax_Preview">Ax=b</span><script type="math/tex">Ax=b</script></span> inside the affine space <span><span class="MathJax_Preview">x_0+K_m</span><script type="math/tex">x_0+K_m</script></span> by imposing that <span><span class="MathJax_Preview">b-Ax \bot \mathcal{L}_m</span><script type="math/tex">b-Ax \bot \mathcal{L}_m</script></span>, where <span><span class="MathJax_Preview">K_m</span><script type="math/tex">K_m</script></span> is Krylov subspace of dimension <span><span class="MathJax_Preview">m</span><script type="math/tex">m</script></span> defined by <span><span class="MathJax_Preview">K_m=\{r_0, Ar_0, A^2r_0,...,A^{m-1}r_0\}</span><script type="math/tex">K_m=\{r_0, Ar_0, A^2r_0,...,A^{m-1}r_0\}</script></span> and <span><span class="MathJax_Preview">\mathcal{L}_m</span><script type="math/tex">\mathcal{L}_m</script></span> is another subspace of dimension <span><span class="MathJax_Preview">m</span><script type="math/tex">m</script></span> which depends on type of Krylov subspace. For example in GMRES, <span><span class="MathJax_Preview">\mathcal{L}_m=AK_m</span><script type="math/tex">\mathcal{L}_m=AK_m</script></span>.</p>
<p>We realized an interface which is easy to use, so that the call of these different softwares in <strong><code>FreeFem++</code></strong> is done in the same way. You just have to load the solver and then specify the parameters to apply to the specific solvers. In the rest of this chapter, when we talk about Krylov subspace methods we mean one among GMRES, CG and BICGSTAB.</p>
<h4 id="parms-solver">pARMS solver<a class="headerlink" href="#parms-solver" title="Permanent link">#</a></h4>
<p><a href="http://www-users.cs.umn.edu/~saad/software/pARMS/">pARMS</a> (parallel Algebraic Multilevel Solver) is a software developed by Youssef Saad and al at University of Minnesota.</p>
<p>This software is specialized in the resolution of large sparse non symmetric linear systems of equation. Solvers developed in pARMS are of type "Krylov's subspace".</p>
<p>It consists of variants of GMRES like FGMRES (Flexible GMRES), DGMRES (Deflated GMRES) <a href="#SAAD2003">SAAD2003</a> and BICGSTAB. pARMS also implements parallel preconditioner like RAS (Restricted Additive Schwarz) <a href="#CAI1989">CAI1989</a> and Schur Complement type preconditioner.</p>
<p>All these parallel preconditioners are based on the principle of domain decomposition. Thus, the matrix <span><span class="MathJax_Preview">A</span><script type="math/tex">A</script></span> is partitioned into sub matrices <span><span class="MathJax_Preview">A_i</span><script type="math/tex">A_i</script></span>(<span><span class="MathJax_Preview">i=1,...,p</span><script type="math/tex">i=1,...,p</script></span>) where p represents the number of partitions one needs. The union of <span><span class="MathJax_Preview">A_i</span><script type="math/tex">A_i</script></span> forms the original matrix. The solution of the overall system is obtained by solving the local systems on <span><span class="MathJax_Preview">A_i</span><script type="math/tex">A_i</script></span> (see <a href="#SMITH1996">SMITH1996</a>). Therefore, a distinction is made between iterations on <span><span class="MathJax_Preview">A</span><script type="math/tex">A</script></span> and the local iterations on <span><span class="MathJax_Preview">A_i</span><script type="math/tex">A_i</script></span>.</p>
<p>To solve the local problem on <span><span class="MathJax_Preview">A_i</span><script type="math/tex">A_i</script></span> there are several preconditioners as <strong>ilut</strong> (Incomplete LU with threshold), <strong>iluk</strong> (Incomplete LU with level of fill in) and <strong>ARMS</strong> (Algebraic Recursive Multilevel Solver).</p>
<div class="admonition example">
<p class="admonition-title">Default parameters</p>
<div class="codehilite"><pre><span></span>load &quot;parms_FreeFem&quot; //Tell FreeFem that you will use pARMS

// Mesh
border C(t=0, 2*pi){x=cos(t); y=sin(t); label=1;}
mesh Th = buildmesh (C(50));

// Fespace
fespace Vh(Th, P2);
Vh u, v;

// Function
func f= x*y;

// Problem
problem Poisson (u, v, solver=sparsesolver)
    = int2d(Th)(
          dx(u)*dx(v)
        + dy(u)*dy(v)
    )
    + int2d(Th)(
        - f*v
    )
    + on(1, u=0)
    ;

// Solve
real cpu = clock();
Poisson;
cout &lt;&lt; &quot; CPU time = &quot; &lt;&lt; clock()-cpu &lt;&lt; endl;

// Plot
plot(u);
</pre></div>

<p>In line 1, the pARMS dynamic library is loaded with interface <strong><code>FreeFem++</code></strong>. After this, in line 15 we specify that the bilinear form will be solved by the last sparse linear solver load in memory which, in this case, is pARMS.</p>
<p>The parameters used in pARMS in this case are the default one since the user does not have to provide any parameter.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In order to see the plot of a parallel script, run the command <code>FreeFem++-mpi -glut ffglut script.edp</code></p>
</div>
</div>
<p>Here are some default parameters:</p>
<ul>
<li><code>solver=FGMRES</code>,</li>
<li><code>Krylov dimension=30</code>,</li>
<li><code>Maximum of Krylov=1000</code>,</li>
<li><code>Tolerance for convergence=$1e-08$</code>(see book <a href="#SAAD2003">SAAD2003</a> to understand all this parameters),</li>
<li><code>preconditionner=Restricted Additif Schwarz</code> <a href="#CAI1989">CAI1989</a>,</li>
<li><code>Inner Krylov dimension=5</code>,</li>
<li><code>Maximum of inner Krylov dimension=5</code>,</li>
<li><code>Inner preconditionner=ILUK</code>.</li>
</ul>
<p>To specify the parameters to apply to the solver, the user can either give an integer vector for <strong>integer parameters</strong> and real vectors for <strong>real parameters</strong> or provide a <strong>file</strong> which contains those parameters.</p>
<div class="admonition example">
<p class="admonition-title">User specifies parameters inside two vectors</p>
<p>Lets us consider Navier-Stokes example. In this example we solve linear systems coming from discretization of Navier-Stokes equations with pARMS. Parameters of solver is specified by user.</p>
<div class="codehilite"><pre><span></span>load &quot;parms_FreeFem&quot;

// Parameters
real nu = 1.;
int[int] iparm(16);
real[int] dparm(6);
for (int ii = 0; ii &lt; 16; ii++)
    iparm[ii] = -1;
for (int ii = 0; ii &lt; 6; ii++)
    dparm[ii] = -1.0;
iparm[0]=0;

// Mesh
mesh Th = square(10, 10);
int[int] wall = [1, 3];
int inlet = 4;

// Fespace
fespace Vh(Th, [P2, P2, P1]);

// Function
func uc = 1.;

varf Stokes ([u, v, p], [ush, vsh, psh], solver=sparsesolver)
    = int2d(Th)(
          nu*(
              dx(u)*dx(ush)
            + dy(u)*dy(ush)
            + dx(v)*dx(vsh)
            + dy(v)*dy(vsh)
        )
        - p*psh*(1.e-6)
        - p*(dx(ush) + dy(vsh))
        - (dx(u) + dy(v))*psh
    )
    + on(wall, wall, u=0., v=0.)
    + on(inlet, u=uc, v=0)
    ;

matrix AA = Stokes(Vh, Vh);
set(AA, solver=sparsesolver, lparams=iparm, dparams=dparm); //set pARMS as linear solver
real[int] bb = Stokes(0, Vh);
real[int] sol(AA.n);
sol = AA^-1 * bb;
</pre></div>

<p>We need two vectors to specify the parameters of the linear solver. In line 5-6 of the example, we have declared these vectors(<code class="codehilite">int[int] iparm(16); real[int] dparm(6);</code>). In line 7-10 we have initialized these vectors by negative values.</p>
<p>We do this because all parameters values in pARMS are positive and if you do not change the negative values of one entry of this vector, the default value will be set.</p>
<p>In <a href="#Tab4">table 4</a> and <a href="#Tab5">table 5</a>, we have the meaning of different entries of these vectors.</p>
<p><table>
    <thead>
        <tr>
            <th colspan="2"><a name="Tab4">Table 4</a>: Meaning of <code class="codehilite">lparams</code> corresponding variables</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td style="font-weight: bold">Entries of <code class="codehilite">iparm</code></td>
            <td style="font-weight: bold">Significations of each entries</td>
        </tr>
        <tr>
            <td><code>iparm[0]</code></td>
            <td>Krylov subspace methods.<br>Different values for this parameters are specify on <a href="#Tab6">table 7</a></td>
        </tr>
        <tr>
            <td><code>iparm[1]</code></td>
            <td>Preconditionner.<br>Different preconditionners for this parameters are specify on <a href="#Tab7">table 7</a></td>
        </tr>
        <tr>
            <td><code>iparm[2]</code></td>
            <td>Krylov subspace dimension in outer iteration: default value 30</td>
        </tr>
        <tr>
            <td><code>iparm[3]</code></td>
            <td>Maximum of iterations in outer iteration: default value 1000</td>
        </tr>
        <tr>
            <td><code>iparm[4]</code></td>
            <td>Number of level in arms when used.</td>
        </tr>
        <tr>
            <td><code>iparm[5]</code></td>
            <td>Krylov subspace dimension in inner iteration: default value 3</td>
        </tr>
        <tr>
            <td><code>iparm[6]</code></td>
            <td>Maximum of iterations in inner iteration: default value 3</td>
        </tr>
        <tr>
            <td><code>iparm[7]</code></td>
            <td>Symmetric(=1 for symmetric) or unsymmetric matrix:<br>default value 0(unsymmetric matrix)</td>
        </tr>
        <tr>
            <td><code>iparm[8]</code></td>
            <td>Overlap size between different subdomain: default value 0(no overlap)</td>
        </tr>
        <tr>
            <td><code>iparm[9]</code></td>
            <td>Scale the input matrix or not: Default value 1 (Matrix should be scaled)</td>
        </tr>
        <tr>
            <td><code>iparm[10]</code></td>
            <td>Block size in arms when used: default value 20</td>
        </tr>
        <tr>
            <td><code>iparm[11]</code></td>
            <td>lfil0 (ilut, iluk, and arms) : default value 20</td>
        </tr>
        <tr>
            <td><code>iparm[12]</code></td>
            <td>lfil for Schur complement const : default value 20</td>
        </tr>
        <tr>
            <td><code>iparm[13]</code></td>
            <td>lfil for Schur complement const : default value 20</td>
        </tr>
        <tr>
            <td><code>iparm[14]</code></td>
            <td>Multicoloring or not in ILU when used : default value 1</td>
        </tr>
        <tr>
            <td><code>iparm[15]</code></td>
            <td>Inner iteration : default value 0</td>
        </tr>
        <tr>
            <td><code>iparm[16]</code></td>
            <td>Print message when solving:default 0 (no message print).<br>0: no message is print,<br>1: Convergence informations like number of iteration and residual ,<br>2: Timing for a different step like preconditioner<br>3 : Print all informations.</td>
        </tr>
    </tbody>
</table></p>
<p><table>
    <thead>
        <tr>
            <th colspan="2"><a name="Tab5">Table 5</a>: Significations of <code class="codehilite">dparams</code> corresponding variables</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td style="font-weight: bold">Entries of <code class="codehilite">dparm</code></td>
            <td style="font-weight: bold">Significations of each entries</td>
        </tr>
        <tr>
            <td><code>dparm[0]</code></td>
            <td>precision for outer iteration : default value 1e-08</td>
        </tr>
        <tr>
            <td><code>dparm[1]</code></td>
            <td>precision for inner iteration: default value 1e-2</td>
        </tr>
        <tr>
            <td><code>dparm[2]</code></td>
            <td>tolerance used for diagonal domain: : default value 0.1</td>
        </tr>
        <tr>
            <td><code>dparm[3]</code></td>
            <td>drop tolerance droptol0 (ilut, iluk, and arms) : default value 1e-2</td>
        </tr>
        <tr>
            <td><code>dparm[4]</code></td>
            <td>droptol for Schur complement const: default value 1e-2</td>
        </tr>
        <tr>
            <td><code>dparm[5]</code></td>
            <td>droptol for Schur complement const: default value 1e-2</td>
        </tr>
    </tbody>
</table></p>
<p><table>
    <thead>
        <tr>
            <th colspan="2"><a name="Tab6">Table 6</a>: Krylov Solvers in pARMS</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td style="font-weight: bold">Values of <code>iparm[0]</code></td>
            <td style="font-weight: bold">Krylov subspace methods</td>
        </tr>
        <tr>
            <td>0</td>
            <td>FGMRES (Flexible GMRES)</td>
        </tr>
        <tr>
            <td>1</td>
            <td>DGMRES (Deflated GMRES)</td>
        </tr>
        <tr>
            <td>2</td>
            <td>BICGSTAB</td>
        </tr>
    </tbody>
</table></p>
<p><table>
    <thead>
        <tr>
            <th colspan="2"><a name="Tab7">Table 7</a>: Preconditionners in pARMS</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td style="font-weight: bold">Values of <code>iparm[1]</code></td>
            <td style="font-weight: bold">Preconditionners type</td>
        </tr>
        <tr>
            <td>0</td>
            <td>additive Schwartz preconditioner with ilu0 as local preconditioner</td>
        </tr>
        <tr>
            <td>1</td>
            <td>additive Schwartz preconditioner with iluk as local preconditioner</td>
        </tr>
        <tr>
            <td>2</td>
            <td>additive Schwartz preconditioner with ilut as local preconditioner</td>
        </tr>
        <tr>
            <td>3</td>
            <td>additive Schwartz preconditioner with arms as local preconditioner</td>
        </tr>
        <tr>
            <td>4</td>
            <td>Left Schur complement preconditioner with ilu0 as local preconditioner</td>
        </tr>
        <tr>
            <td>5</td>
            <td>Left Schur complement preconditioner with ilut as local preconditioner</td>
        </tr>
        <tr>
            <td>6</td>
            <td>Left Schur complement preconditioner with iluk as local preconditioner</td>
        </tr>
        <tr>
            <td>7</td>
            <td>Left Schur complement preconditioner with arms as local preconditioner</td>
        </tr>
        <tr>
            <td>8</td>
            <td>Right Schur complement preconditioner with ilu0 as local preconditioner</td>
        </tr>
        <tr>
            <td>9</td>
            <td>Right Schur complement preconditioner with ilut as local preconditioner</td>
        </tr>
        <tr>
            <td>10</td>
            <td>Right Schur complement preconditioner with iluk as local preconditioner</td>
        </tr>
        <tr>
            <td>11</td>
            <td>Right Schur complement preconditioner with arms as local preconditioner</td>
        </tr>
        <tr>
            <td>12</td>
            <td>sch_gilu0, Schur complement preconditioner with global ilu0</td>
        </tr>
        <tr>
            <td>13</td>
            <td>SchurSymmetric GS preconditioner</td>
        </tr>
    </tbody>
</table></p>
<p>We run this example on a cluster paradent of Grid5000 and report results in <a href="#Tab8">table 8</a>.</p>
<p><table>
    <thead>
        <tr>
            <th colspan="5"><a name="Tab8">Table 8</a>: Convergence and time for solving linear system</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td colspan="2" align="center" style="font-weight: bold">n=471281</td>
            <td colspan="2" align="center" style="font-weight: bold">nnz=<span><span class="MathJax_Preview">13\times10^6</span><script type="math/tex">13\times10^6</script></span></td>
            <td colspan="2" align="center" style="font-weight: bold">Te=571,29</td>
        </tr>
        <tr>
            <td rowspan="2" align="center" style="vertical-align: bottom !important">np</td>
            <td colspan="2" align="center">add(iluk)</td>
            <td colspan="2" align="center">schur(iluk)</td>
        </tr>
        <tr>
            <td align="center"><code>nit</code></td>
            <td align="center"><code>time</code></td>
            <td align="center"><code>nit</code></td>
            <td align="center"><code>time</code></td>
        </tr>
        <tr>
            <td align="center">4</td>
            <td align="center">230</td>
            <td align="center">637.57</td>
            <td align="center">21</td>
            <td align="center">557.8</td>
        </tr>
        <tr>
            <td align="center">8</td>
            <td align="center">240</td>
            <td align="center">364.12</td>
            <td align="center">22</td>
            <td align="center">302.25</td>
        </tr>
        <tr>
            <td align="center">16</td>
            <td align="center">247</td>
            <td align="center">212.07</td>
            <td align="center">24</td>
            <td align="center">167.5</td>
        </tr>
        <tr>
            <td align="center">32</td>
            <td align="center">261</td>
            <td align="center">111.16</td>
            <td align="center">25</td>
            <td align="center">81.5</td>
        </tr>
    </tbody>
</table></p>
<p><table>
    <thead>
        <tr>
            <th colspan="2"><a name="Ta9">Table 9</a>: Legend of <a href="#Tab8">table 8</a></th>
        </tr>
    </thead>
    <tbody>
    <tr>
        <td><code>n</code></td>
        <td>matrix size</td>
    </tr>
    <tr>
        <td><code>nnz</code></td>
        <td>number of non null entries inside matrix</td>
    </tr>
    <tr>
        <td><code>nit</code></td>
        <td>number of iteration for convergence</td>
    </tr>
    <tr>
        <td><code>time</code></td>
        <td>Time for convergence</td>
    </tr>
    <tr>
        <td><code>Te</code></td>
        <td>Time for constructing finite element matrix</td>
    </tr>
    <tr>
        <td><code>np</code></td>
        <td>number of processor</td>
    </tr>
</table></p>
<p>In this example, we fix the matrix size (in term of finite element, we fix the mesh) and increase the number of processors used to solve the linear system. We saw that, when the number of processors increases, the time for solving the linear equation decreases, even if the number of iteration increases. This proves that, using pARMS as solver of linear systems coming from discretization of partial differential equation in <strong><code>FreeFem++</code></strong> can decrease drastically the total time of simulation.</p>
</div>
<h4 id="interfacing-with-hips">Interfacing with HIPS<a class="headerlink" href="#interfacing-with-hips" title="Permanent link">#</a></h4>
<p><a href="http://hips.gforge.inria.fr/">HIPS</a> (<em>Hierarchical Iterative Parallel Solver</em>) is a scientific library that provides an efficient parallel iterative solver for very large sparse linear systems. HIPS is available as free software under the CeCILL-C licence.</p>
<p>HIPS implements two solver classes which are the iteratives class (GMRES, PCG) and the Direct class. Concerning preconditionners, HIPS implements a type of multilevel ILU. For further informations on those preconditionners see the <a href="http://hips.gforge.inria.fr/doc/hips_user.pdf">HIPS documentation</a>.</p>
<div class="admonition example">
<p class="admonition-title">Laplacian 3D solved with HIPS</p>
<p>Let us consider the 3D Laplacian example inside <strong><code>FreeFem++</code></strong> package where after discretization we want to solve the linear equation with HIPS.</p>
<p>The following example is a Laplacian 3D using Hips as linear solver. We first load Hips solver at line 2. From line 7 to 18 we specify the parameters for the Hips solver and in line 82 we set these parameters in the linear solver.</p>
<p>In <a href="#Tab10">Table 10</a> results of running on Cluster Paradent of Grid5000 are reported. We can see in this running example the efficiency of parallelism.</p>
<div class="codehilite"><pre><span></span>load &quot;msh3&quot;
load &quot;hips_FreeFem&quot; //load Hips library

// Parameters
int nn = 10;
real zmin = 0, zmax = 1;
int[int] iparm(14);
real[int] dparm(6);
for (int iii = 0; iii &lt; 14; iii++) iparm[iii] = -1;
for (int iii = 0; iii &lt; 6; iii++) dparm[iii] = -1;
iparm[0] = 0; //use iterative solver
iparm[1] = 1; //PCG as Krylov method
iparm[4] = 0; //Matrix are symmetric
iparm[5] = 1; //Pattern are also symmetric
iparm[9] = 1; //Scale matrix
dparm[0] = 1e-13; //Tolerance to convergence
dparm[1] = 5e-4; //Threshold in ILUT
dparm[2] = 5e-4; //Threshold for Schur preconditionner

// Functions
func ue = 2*x*x + 3*y*y + 4*z*z + 5*x*y + 6*x*z + 1;
func uex = 4*x + 5*y + 6*z;
func uey = 6*y + 5*x;
func uez = 8*z + 6*x;
func f = -18.;

// Mesh
mesh Th2 = square(nn, nn);

int[int] rup = [0,2], rdown=[0, 1];
int[int] rmid=[1, 1, 2, 1, 3, 1, 4, 1];

mesh3 Th=buildlayers(Th2, nn,
    zbound=[zmin, zmax],
    reffacemid=rmid,
    reffaceup = rup,
    reffacelow = rdown);

// Fespace
fespace Vh2(Th2, P2);
Vh2 ux, uz, p2;

fespace Vh(Th, P2);
Vh uhe = ue;
cout &lt;&lt; &quot;uhe min = &quot; &lt;&lt; uhe[].min &lt;&lt; &quot;, max = &quot; &lt;&lt; uhe[].max &lt;&lt; endl;
Vh u, v;
Vh F;

// Macro
macro Grad3(u) [dx(u), dy(u), dz(u)] //

// Problem
varf va (u, v)
    = int3d(Th)(
          Grad3(v)&#39; * Grad3(u)
    )
    + int2d(Th, 2)(
          u*v
    )
    - int3d(Th)(
          f*v
    )
    - int2d(Th, 2)(
          ue*v
        + (uex*N.x + uey*N.y + uez*N.z)*v
    )
    + on(1, u=ue);

varf l (unused, v) = int3d(Th)(f*v);

real cpu=clock();
matrix Aa = va(Vh, Vh);

F[] = va(0, Vh);

if (mpirank == 0){
    cout &lt;&lt; &quot;Size of A = &quot; &lt;&lt; Aa.n &lt;&lt; endl;
    cout &lt;&lt; &quot;Non zero coefficients = &quot; &lt;&lt; Aa.nbcoef &lt;&lt; endl;
    cout &lt;&lt; &quot;CPU TIME FOR FORMING MATRIX = &quot; &lt;&lt; clock()-cpu &lt;&lt; endl;
}

set(Aa, solver=sparsesolver, dparams=dparm, lparams=iparm); //Set hips as linear solver

// Solve
u[] = Aa^-1*F[];

// Plot
plot(u);
</pre></div>

<p><table>
    <thead>
        <tr>
            <th colspan="3"><a name="Tab10">Table 10</a>: Legend of this table are give in <a href="#Tab9">table 9</a></th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td align='center'><span><span class="MathJax_Preview">n=4 \times 10^6</span><script type="math/tex">n=4 \times 10^6</script></span></td>
            <td align='center'><span><span class="MathJax_Preview">nnz=118 \times 10^6</span><script type="math/tex">nnz=118 \times 10^6</script></span></td>
            <td align='center'><span><span class="MathJax_Preview">Te=221.34</span><script type="math/tex">Te=221.34</script></span></td>
        </tr>
        <tr>
            <td align='center'><code>np</code></td>
            <td align='center'><code>nit</code></td>
            <td align='center'><code>time</code></td>
        </tr>
        <tr>
            <td align='center'>8</td>
            <td align='center'>190</td>
            <td align='center'>120.34</td>
        </tr>
        <tr>
            <td align='center'>16</td>
            <td align='center'>189</td>
            <td align='center'>61.08</td>
        </tr>
        <tr>
            <td align='center'>32</td>
            <td align='center'>186</td>
            <td align='center'>31.70</td>
        </tr>
        <tr>
            <td align='center'>64</td>
            <td align='center'>183</td>
            <td align='center'>23.44</td>
        </tr>
    </tbody>
</table></p>
</div>
<div class="admonition tips">
<p class="admonition-title">Tips</p>
<p><table>
    <thead>
        <tr>
            <th colspan="2"><a name="Tab11">Table 11</a>: Significations of <code class="codehilite">lparams</code> corresponding to HIPS interface</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td style="font-weight: bold">Entries of <code>iparm</code></td>
            <td style="font-weight: bold">Significations of each entries</td>
        </tr>
        <tr>
            <td><code>iparm[0]</code></td>
            <td>Strategy use for solving (Iterative=0 or Hybrid=1 or Direct=2). Defaults values are : Iterative</td>
        </tr>
        <tr>
            <td><code>iparm[1]</code></td>
            <td>Krylov methods. If iparm[0]=0, give type of Krylov methods: 0 for GMRES, 1 for PCG</td>
        </tr>
        <tr>
            <td><code>iparm[2]</code></td>
            <td>Maximum of iterations in outer iteration: default value 1000</td>
        </tr>
        <tr>
            <td><code>iparm[3]</code></td>
            <td>Krylov subspace dimension in outer iteration: default value 40</td>
        </tr>
        <tr>
            <td><code>iparm[4]</code></td>
            <td>Symmetric(=0 for symmetric) and 1 for unsymmetricmatrix: default value 1 (unsymmetric matrix)</td>
        </tr>
        <tr>
            <td><code>iparm[5]</code></td>
            <td>Pattern of matrix are symmetric or not: default value 0</td>
        </tr>
        <tr>
            <td><code>iparm[6]</code></td>
            <td>Partition type of input matrix: default value 0</td>
        </tr>
        <tr>
            <td><code>iparm[7]</code></td>
            <td>Number of level that use the HIPS locally consistentfill-in: Default value 2</td>
        </tr>
        <tr>
            <td><code>iparm[8]</code></td>
            <td>Numbering in indices array will start at 0 or 1: Default value 0</td>
        </tr>
        <tr>
            <td><code>iparm[9]</code></td>
            <td>Scale matrix. Default value 1</td>
        </tr>
        <tr>
            <td><code>iparm[10]</code></td>
            <td>Reordering use inside subdomains for reducingfill-in: Only use for iterative. Default value 1</td>
        </tr>
        <tr>
            <td><code>iparm[11]</code></td>
            <td>Number of unknowns per node in the matrix non-zeropattern graph: Default value 1</td>
        </tr>
        <tr>
            <td><code>iparm[12]</code></td>
            <td>This value is used to set the number of time the normalization is applied to the matrix: Default 2.</td>
        </tr>
        <tr>
            <td><code>iparm[13]</code></td>
            <td>Level of informations printed during solving: Default 5.</td>
        </tr>
        <tr>
            <td><code>iparm[14]</code></td>
            <td>HIPS_DOMSIZE Subdomain size</td>
        </tr>
    </tbody>
</table></p>
<p><table>
    <thead>
        <tr>
            <th colspan="2"><a name="Tab12">Table 12</a>: Significations of <code class="codehilite">dparams</code> corresponding to HIPS interface</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td><code>dparm[0]</code></td>
            <td>HIPS_PREC: Relative residual norm: Default=1e-9</td>
        </tr>
        <tr>
            <td><code>dparm[1]</code></td>
            <td>HIPS_DROPTOL0: Numerical threshold in ILUT for interior domain (important : set 0.0 in HYBRID: Default=0.005)</td>
        </tr>
        <tr>
            <td><code>dparm[2]</code></td>
            <td>HIPS_DROPTOL1 : Numerical threshold in ILUT for Schur preconditioner: Default=0.005</td>
        </tr>
        <tr>
            <td><code>dparm[3]</code></td>
            <td>HIPS_DROPTOLE : Numerical threshold for coupling between the interior level and Schur: Default 0.005</td>
        </tr>
        <tr>
            <td><code>dparm[4]</code></td>
            <td>HIPS_AMALG : Numerical threshold for coupling between the interior level and Schur: Default=0.005</td>
        </tr>
        <tr>
            <td><code>dparm[5]</code></td>
            <td>HIPS_DROPSCHUR : Numerical threshold for coupling between the interior level and Schur: Default=0.005</td>
        </tr>
    </tbody>
</table></p>
</div>
<h4 id="interfacing-with-hypre">Interfacing with HYPRE<a class="headerlink" href="#interfacing-with-hypre" title="Permanent link">#</a></h4>
<p><a href="https://computation.llnl.gov/projects/hypre-scalable-linear-solvers-multigrid-methods">Hypre</a> (High Level Preconditioner) is a suite of parallel preconditioner developed at Lawrence Livermore National Lab.</p>
<p>There are two main classes of preconditioners developed in HYPRE: AMG (Algebraic MultiGrid) and Parasails (Parallel Sparse Approximate Inverse).</p>
<p>Now, suppose we want to solve <span><span class="MathJax_Preview">Ax=b</span><script type="math/tex">Ax=b</script></span>.</p>
<p>At the heart of AMG there is a series of progressively coarser (smaller) representations of the matrix <span><span class="MathJax_Preview">A</span><script type="math/tex">A</script></span>. Given an approximation <span><span class="MathJax_Preview">\hat{x}</span><script type="math/tex">\hat{x}</script></span> to the solution <span><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span>, consider solving the residual equation <span><span class="MathJax_Preview">Ae=r</span><script type="math/tex">Ae=r</script></span> to find the error <span><span class="MathJax_Preview">e</span><script type="math/tex">e</script></span>, where <span><span class="MathJax_Preview">r=b-A\hat{x}</span><script type="math/tex">r=b-A\hat{x}</script></span>. A fundamental principle of AMG is that it is an algebraically smooth error. To reduce the algebraically smooth errors further, they need to be represented by a smaller defect equation (coarse grid residual equation) <span><span class="MathJax_Preview">A_ce_c=r_c</span><script type="math/tex">A_ce_c=r_c</script></span>, which is cheaper to solve. After solving this coarse equation, the solution is then interpolated in fine grid represented here by matrix <span><span class="MathJax_Preview">A</span><script type="math/tex">A</script></span>. The quality of AMG depends on the choice of coarsening and interpolating operators.</p>
<p>The <em>sparse approximate inverse</em> approximates the inverse of a matrix <span><span class="MathJax_Preview">A</span><script type="math/tex">A</script></span> by a sparse matrix <span><span class="MathJax_Preview">M</span><script type="math/tex">M</script></span>. A technical idea to construct matrix <span><span class="MathJax_Preview">M</span><script type="math/tex">M</script></span> is to minimize the Frobenuis norm of the residual matrix <span><span class="MathJax_Preview">I-MA</span><script type="math/tex">I-MA</script></span>. For more details on this preconditioner technics see <a href="#CHOW1997">CHOW1997</a>.</p>
<p>HYPRE implement three Krylov subspace solvers: GMRES, PCG and BiCGStab.</p>
<div class="admonition example">
<p class="admonition-title">Laplacian 3D solved with HYPRE</p>
<p>Let us consider again the 3D Laplacian example inside FreeFem++ package where after discretization we want to solve the linear equation with Hypre. The following example is a Laplacian 3D using Hypre as linear solver. This is the same example as Hips one, so we just show here the lines where we set some Hypre parameters.</p>
<p>We first load the Hypre solver at line 2. From line 6 to 18 we specifies the parameters to set to Hypre solver and in line 22 we set parameters to Hypre solver.</p>
<p>It should be noted that the meaning of the entries of these vectors is different from those of Hips. In the case of HYPRE, the meaning of differents entries of vectors <code class="codehilite">iparm</code> and <code class="codehilite">dparm</code> are given in <a href="#Tab13">tables 13</a> to <a href="#Tab17">17</a>.</p>
<p>In <a href="#Tab18">Table 18</a> the results of running on Cluster Paradent of Grid5000 are reported. We can see in this running example the efficiency of parallelism, in particular when AMG are use as preconditioner.</p>
<div class="codehilite"><pre><span></span>load &quot;msh3&quot;
load &quot;hipre_FreeFem&quot; //Load Hipre librairy

// Parameters
int nn = 10;
int[int] iparm(20);
real[int] dparm(6);
for (int iii = 0; iii &lt; 20; iii++) iparm[iii] = -1;
for (int iii = 0; iii &lt; 6; iii++) dparm[iii] = -1;
iparm[0] = 2; //PCG as krylov method
iparm[1] = 0; //AMG as preconditionner 2: if ParaSails
iparm[7] = 7; //Interpolation
iparm[9] = 6; //AMG Coarsen type
iparm[10] = 1; //Measure type
iparm[16] = 2; //Additive schwarz as smoother
dparm[0] = 1e-13; //Tolerance to convergence
dparm[1] = 5e-4; //Threshold
dparm[2] = 5e-4; //Truncation factor

...

set(Aa, solver=sparsesolver, dparams=dparm, lparams=iparm);
</pre></div>

<p><table>
    <thead>
        <tr>
            <th colspan="2"><a name="Tab13">Table 13</a>: Definitions of common entries of <code class="codehilite">iparms</code> and <code class="codehilite">dparms</code> vectors for every preconditioner in HYPRE</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td><code>iparms[0]</code></td>
            <td>Solver identification: 0: BiCGStab, 1: GMRES, 2: PCG. Default=1</td>
        </tr>
        <tr>
            <td><code>iparms[1]</code></td>
            <td>Preconditioner identification: 0: BOOMER AMG, 1: PILUT, 2: Parasails, 3: Schwartz Default=0</td>
        </tr>
        <tr>
            <td><code>iparms[2]</code></td>
            <td>Maximum of iteration: Default=1000</td>
        </tr>
        <tr>
            <td><code>iparms[3]</code></td>
            <td>Krylov subspace dim: Default= 40</td>
        </tr>
        <tr>
            <td><code>iparms[4]</code></td>
            <td>Solver print info level: Default=2</td>
        </tr>
        <tr>
            <td><code>iparms[5]</code></td>
            <td>Solver log: Default=1</td>
        </tr>
        <tr>
            <td><code>iparms[6]</code></td>
            <td>Solver stopping criteria only for BiCGStab : Default=1</td>
        </tr>
        <tr>
            <td><code>dparms[0]</code></td>
            <td>Tolerance for convergence: Default=<span><span class="MathJax_Preview">1.0e-11</span><script type="math/tex">1.0e-11</script></span></td>
        </tr>
    </tbody>
</table></p>
<p><table>
    <thead>
        <tr>
            <th colspan="2"><a name="Tab14">Table 14</a>: Definitions of other entries of <code class="codehilite">iparms</code> and <code class="codehilite">dparms</code> if preconditioner is BOOMER AMG</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td><code>iparms[7]</code></td>
            <td>AMG interpolation type: Default=6</td>
        </tr>
        <tr>
            <td><code>iparms[8]</code></td>
            <td>Specifies the use of GSMG - geometrically smooth coarsening and interpolation: Default=1</td>
        </tr>
        <tr>
            <td><code>iparms[9]</code></td>
            <td>AMG coarsen type: Default=6</td>
        </tr>
        <tr>
            <td><code>iparms[10]</code></td>
            <td>Defines whether local or global measures are used: Default=1</td>
        </tr>
        <tr>
            <td><code>iparms[11]</code></td>
            <td>AMG cycle type: Default=1</td>
        </tr>
        <tr>
            <td><code>iparms[12]</code></td>
            <td>AMG Smoother type: Default=1</td>
        </tr>
        <tr>
            <td><code>iparms[13]</code></td>
            <td>AMG number of levels for smoothers: Default=3</td>
        </tr>
        <tr>
            <td><code>iparms[14]</code></td>
            <td>AMG number of sweeps for smoothers: Default=2</td>
        </tr>
        <tr>
            <td><code>iparms[15]</code></td>
            <td>Maximum number of multigrid levels: Default=25</td>
        </tr>
        <tr>
            <td><code>iparms[16]</code></td>
            <td>Defines which variant of the Schwartz method isused:<br>
            0: hybrid multiplicative Schwartz method (no overlap across processor boundaries)<br>
            1: hybrid additive Schwartz method (no overlap across processor boundaries)<br>
            2: additive Schwartz method<br>
            3: hybrid multiplicative Schwartz method (with overlap across processor boundaries)<br>
            Default=1
            </td>
        </tr>
        <tr>
            <td><code>iparms[17]</code></td>
            <td>Size of the system of PDEs: Default=1</td>
        </tr>
        <tr>
            <td><code>iparms[18]</code></td>
            <td>Overlap for the Schwarz method: Default=1</td>
        </tr>
        <tr>
            <td>Type of domain used for the Schwarz method<br>
            <td><code>iparms[19]</code></td>
            0: each point is a domain<br>
            1: each node is a domain (only of interest in "systems" AMG)<br>
            2: each domain is generated by agglomeration (default)</td>
        </tr>
        <tr>
            <td><code>dparms[1]</code></td>
            <td>AMG strength threshold: Default=0.25</td>
        </tr>
        <tr>
            <td><code>dparms[2]</code></td>
            <td>Truncation factor for the interpolation: Default=1e-2</td>
        </tr>
        <tr>
            <td><code>dparms[3]</code></td>
            <td>Sets a parameter to modify the definition of strength for diagonal dominant portions of the matrix: Default=0.9</td>
        </tr>
        <tr>
            <td><code>dparms[3]</code></td>
            <td>Defines a smoothing parameter for the additive Schwartz method. Default=1</td>
        </tr>
    </tbody>
</table></p>
<p><table>
    <thead>
        <tr>
            <th colspan="2"><a name="Tab15">Table 15</a>: Definitions of other entries of <code class="codehilite">iparms</code> and <code class="codehilite">dparms</code> if preconditioner is PILUT</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td><code>iparms[7]</code></td>
            <td>Row size in Parallel ILUT: Default=1000</td>
        </tr>
        <tr>
            <td><code>iparms[8]</code></td>
            <td>Set maximum number of iterations: Default=30</td>
        </tr>
        <tr>
            <td><code>dparms[1]</code></td>
            <td>Drop tolerance in Parallel ILUT: Default=<span><span class="MathJax_Preview">1e-5</span><script type="math/tex">1e-5</script></span></td>
        </tr>
    </tbody>
</table></p>
<p><table>
    <thead>
        <tr>
            <th colspan="2"><a name="Tab16">Table 16</a>: Definitions of other entries of <code class="codehilite">iparms</code> and <code class="codehilite">dparms</code> if preconditioner is ParaSails</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td><code>iparms[7]</code></td>
            <td>Number of levels in Parallel Sparse Approximate inverse: Default=1</td>
        </tr>
        <tr>
            <td><code>iparms[8]</code></td>
            <td>Symmetric parameter for the ParaSails preconditioner:<br>
            0: nonsymmetric and/or indefinite problem, and nonsymmetric preconditioner<br>
            1: SPD problem, and SPD (factored) preconditioner<br>
            2: nonsymmetric, definite problem, and SPD (factored) preconditioner<br>
            Default=0</td>
        </tr>
        <tr>
            <td><code>dparms[1]</code></td>
            <td>Filters parameters. The filter parameter is used to drop small nonzeros in the preconditioner, to reduce the cost of applying the preconditioner: Default=0.1</td>
        </tr>
        <tr>
            <td><code>dparms[2]</code></td>
            <td>Threshold parameter: Default=0.1</td>
        </tr>
    </tbody>
</table></p>
<p><table>
    <thead>
        <tr>
            <th colspan="2"><a name="Tab17">Table 17</a>: Definitions of other entries of <code class="codehilite">iparms</code> and <code class="codehilite">dparms</code> if preconditionner is Schwartz</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td><code>iparms[7]</code></td>
            <td>Defines which variant of the Schwartz method isused:<br>
            0: hybrid multiplicative Schwartz method (no overlap across processor boundaries)<br>
            1: hybrid additive Schwartz method (no overlap across processor boundaries)<br>
            2: additive Schwartz method<br>
            3: hybrid multiplicative Schwartz method (with overlap across processor boundaries)<br>
            Default=1</td>
        </tr>
        <tr>
            <td><code>iparms[8]</code></td>
            <td>Overlap for the Schwartz method: Default=1</td>
        </tr>
        <tr>
            <td><code>iparms[9]</code></td>
            <td>Type of domain used for the Schwartz method<br>
            0: each point is a domain<br>
            1: each node is a domain (only of interest in "systems" AMG)<br>
            2: each domain is generated by agglomeration (default)</td>
        </tr>
    </tbody>
</table></p>
<p><table>
    <thead>
        <tr>
            <th colspan="3"><a name="Tab18">Table 18</a>: Convergence and time for solving linear system</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td align='center'>n = <span><span class="MathJax_Preview">4\times10^6</span><script type="math/tex">4\times10^6</script></span></td>
            <td align='center'>nnz = <span><span class="MathJax_Preview">13\times10^6</span><script type="math/tex">13\times10^6</script></span></td>
            <td align='center'><span><span class="MathJax_Preview">Te = 571,29</span><script type="math/tex">Te = 571,29</script></span></td>
        </tr>
        <tr>
            <td rowspan="2" align='center'>np</td>
            <td colspan="2" align='center'>AMG</td>
        </tr>
        <tr>
            <td align='center'><code>nit</code></td>
            <td align='center'><code>time</code></td>
        </tr>
        <tr>
            <td align='center'>8</td>
            <td align='center'>6</td>
            <td align='center'>1491.83</td>
        </tr>
        <tr>
            <td align='center'>16</td>
            <td align='center'>5</td>
            <td align='center'>708.49</td>
        </tr>
        <tr>
            <td align='center'>32</td>
            <td align='center'>4</td>
            <td align='center'>296.22</td>
        </tr>
        <tr>
            <td align='center'>64</td>
            <td align='center'>4</td>
            <td align='center'>145.64</td>
        </tr>
    </tbody>
</table></p>
</div>
<h4 id="conclusion">Conclusion<a class="headerlink" href="#conclusion" title="Permanent link">#</a></h4>
<p>With the different runs presented here, we wanted to illustrate the gain in time when we increase the number of processors used for the simulations. We saw that in every case the time for the construction of the finite element matrix is constant. This is normal because until now this phase is sequential in FreeFem++. In contrast, phases for solving the linear system are parallel. We saw on several examples presented here that when we increase the number of processors, in general we decrease the time used for solving the linear systems. But this is not true in every case. In several case, when we increase the number of processors the time to convergence also increases. There are two main reasons for this. First, the increase of processors can lead to the increase of volume of exchanged data across processors consequently increasing the time for solving the linear systems.</p>
<p>Furthermore, in decomposition domain type preconditioners, the number of processors generally corresponds to the number of sub domains. In subdomain methods, generally when we increase the number of subdomains we decrease convergence quality of the preconditioner. This can increase the time used for solving linear equations.</p>
<p>To end this, we should note that good use of the preconditioners interfaced in <strong><code>FreeFem++</code></strong> is empiric, because it is difficult to know what is a good preconditioner for some type of problems. Although, the efficiency of preconditioners sometimes depends on how its parameters are set. For this reason we advise the user to pay attention to the meaning of the parameters in the user guide of the iterative solvers interfaced in <strong><code>FreeFem++</code></strong>.</p>
<h3 id="domain-decomposition">Domain decomposition<a class="headerlink" href="#domain-decomposition" title="Permanent link">#</a></h3>
<p>In the previous section, we saw that the phases to construct a matrix are sequential. One strategy to construct the matrix in parallel is to divide geometrically the domain into subdomains. In every subdomain we construct a local submatrix and after that we assemble every submatrix to form the global matrix.</p>
<p>We can use this technique to solve PDE directly in domain <span><span class="MathJax_Preview">\Omega</span><script type="math/tex">\Omega</script></span>. In this case, in every subdomains you have to define artificial boundary conditions to form consistent equations in every subdomains. After this, you solve equation in every subdomains and define a strategy to obtain the global solution.</p>
<p>In terms of parallel programming for <strong><code>FreeFem++</code></strong>, with MPI, this means that the user must be able to divide processors avaible for computation into subgroups of processors and also must be able to realize different type of communications in <strong><code>FreeFem++</code></strong> script. Here is a wrapper of some MPI functions.</p>
<h4 id="communicators-and-groups">Communicators and groups<a class="headerlink" href="#communicators-and-groups" title="Permanent link">#</a></h4>
<p><strong>Groups</strong></p>
<p><code class="codehilite">mpiGroup grpe(mpiGroup gp, KN_&lt;long&gt;)</code>: Create MPI_Group from existing group <code class="codehilite">gp</code> by
given vector.</p>
<p><strong>Communicators</strong></p>
<p>Communicators is an abstract MPI object which allows MPI user to communicate across group of processors. Communicators can be Intra-communicators(involves a single group) or Inter-communicators (involves two groups). When we not specify type of communicator it will be Intra-communicators</p>
<p><strong>mpiComm cc(mpiComm comm, mpiGroup gp):</strong> Creates a new communicator.</p>
<p><code class="codehilite">comm</code> communicator(handle), <code class="codehilite">gp</code> group which is a subset of the group of <code class="codehilite">comm</code> (handle). Return new communicator</p>
<p><strong>mpiComm cc(mpiGroup gp)</strong>: Same as previous constructor but default <code class="codehilite">comm</code> here is <code>MPI_COMM_WORLD</code>.</p>
<p><strong>mpiComm cc(mpiComm comm, int color, int key):</strong> Creates new communicators based on <code class="codehilite">colors</code> and <code class="codehilite">key</code>. This constructor is based on MPI_Comm_split routine of MPI.</p>
<p><strong>mpiComm cc(MPIrank p, int key):</strong> Same constructor than the last one.</p>
<p>Here <code class="codehilite">colors</code> and <code class="codehilite">comm</code> is defined in <code class="codehilite">MPIrank</code>. This constructor is based on <code>MPI_Comm_split</code> routine of MPI.</p>
<div class="admonition example">
<p class="admonition-title">Split communicator</p>
<div class="codehilite"><pre><span></span>mpiComm comm(mpiCommWorld, 0, 0);
int color = mpiRank(comm)%2;
mpiComm ccc(processor(color, comm), 0);
mpiComm qpp(comm, 0, 0);
mpiComm cp(ccc, color, 0);
</pre></div>

</div>
<p><strong>mpiComm cc(mpiComm comm, int high):</strong> Creates an intracommunicator from an intercommunicator. <code class="codehilite">comm</code> intercommunicator, <code class="codehilite">high</code>.</p>
<p>Used to order the groups within <code class="codehilite">comm</code> (logical) when creating the new communicator. This constructor is based on <code>MPI_Intercomm_merge</code> routine of MPI.</p>
<p><strong>mpiComm cc(MPIrank p1, MPIrank p2, int tag):</strong> This constructor creates an intercommuncator from two intracommunicators. <code class="codehilite">p1</code> defined local (intra)communicator and rank in <code>local_comm</code> of leader (often 0) while <code class="codehilite">p2</code> defined remote communicator and rank in <code>peer_comm</code> of remote leader (often 0). <code class="codehilite">tag</code> Message tag to use in constructing intercommunicator. This constructor is based on <code>MPI_Intercomm_create</code>.</p>
<div class="admonition example">
<p class="admonition-title">Merge</p>
<div class="codehilite"><pre><span></span>mpiComm comm, cc;
int color = mpiRank(comm)%2;
int rk = mpiRank(comm);
int size = mpiSize(comm);
cout &lt;&lt; &quot;Color values: &quot; &lt;&lt; color &lt;&lt; endl;
mpiComm ccc(processor((rk&lt;size/2), comm), rk);
mpiComm cp(cc, color, 0);
int rleader;
if (rk == 0){ rleader = size/2; }
else if (rk == size/2){ rleader = 0; }
else{ rleader = 3; }
mpiComm qqp(processor(0, ccc), processor(rleader, comm), 12345);
int aaa = mpiSize(ccc);
cout &lt;&lt; &quot;Number of processor: &quot; &lt;&lt; aaa &lt;&lt; endl;
</pre></div>

</div>
<h4 id="process">Process<a class="headerlink" href="#process" title="Permanent link">#</a></h4>
<p>In <strong><code>FreeFem++</code></strong> we wrap MPI process by function call <code class="codehilite">processor</code> which create internal <strong><code>FreeFem++</code></strong> object call <code class="codehilite">MPIrank</code>. This mean that do not use <code class="codehilite">MPIrank</code> in <strong><code>FreeFem++</code></strong> script.</p>
<p><code class="codehilite">processor(int rk)</code>: Keep process rank inside object <code class="codehilite">MPIrank</code>. Rank is inside <code>MPI_COMM_WORLD</code>.</p>
<p><code class="codehilite">processor(int rk, mpiComm cc)</code> and <code class="codehilite">processor(mpiComm cc, int rk)</code> process rank inside communicator cc.</p>
<p><code class="codehilite">processor(int rk, mpiComm cc)</code> and <code class="codehilite">processor(mpiComm cc, int rk)</code> process rank inside communicator cc.</p>
<p><code class="codehilite">processorblock(int rk)</code>: This function is exactlly the same than <code class="codehilite">processor(int rk)</code> but is use in case of blocking communication.</p>
<p><code class="codehilite">processorblock(int rk, mpiComm cc)</code>: This function is exactly the same as <code class="codehilite">processor(int rk, mpiComm cc)</code> but uses a synchronization point.</p>
<h4 id="points-to-points-communicators">Points to Points communicators<a class="headerlink" href="#points-to-points-communicators" title="Permanent link">#</a></h4>
<p>In <strong><code>FreeFem++</code></strong> you can call MPI points to points communications functions.</p>
<p><code class="codehilite">Send(processor(int rk, mpiComm cc), Data D)</code> : Blocking send of <code class="codehilite">Data D</code> to processor of <code class="codehilite">rank rk</code> inside communicator <code class="codehilite">cc</code>. Note that <code class="codehilite">Data D</code> can be: <code class="codehilite">int</code>, <code class="codehilite">real</code>, <code class="codehilite">complex</code>, <code class="codehilite">int[int]</code>, <code class="codehilite">real[int]</code>, <code class="codehilite">complex[int]</code>, <code class="codehilite">Mesh</code>, <code class="codehilite">Mesh3</code>, <code class="codehilite">Matrix</code>.</p>
<p><code class="codehilite">Recv(processor(int rk, mpiComm cc), Data D)</code>: Receive <code class="codehilite">Data D</code> from process of rank <code class="codehilite">rk</code> in communicator <code class="codehilite">cc</code>. Note that <code class="codehilite">Data D</code> can be: <code class="codehilite">int</code>, <code class="codehilite">real</code>, <code class="codehilite">complex</code>, <code class="codehilite">int[int]</code>, <code class="codehilite">real[int]</code>, <code class="codehilite">complex[int]</code>, <code class="codehilite">Mesh</code>, <code class="codehilite">Mesh3</code>, <code class="codehilite">Matrix</code> and should be the same type than corresponding send.</p>
<p><code class="codehilite">Isend(processor(int rk, mpiComm cc), Data D)</code> : Non blocking send of <code class="codehilite">Data D</code> to processor of <code class="codehilite">rank rk</code> inside communicator <code class="codehilite">cc</code>. Note that <code class="codehilite">Data D</code> can be: <code class="codehilite">int</code>, <code class="codehilite">real</code>, <code class="codehilite">complex</code>, <code class="codehilite">int[int]</code>, <code class="codehilite">real[int]</code>, <code class="codehilite">complex[int]</code>, <code class="codehilite">mesh</code>, <code class="codehilite">mesh3</code>, <code class="codehilite">matrix</code>.</p>
<p><code class="codehilite">Recv(processor(int rk, mpiComm cc), Data D)</code>: Receive corresponding to send.</p>
<h4 id="global-operations">Global operations<a class="headerlink" href="#global-operations" title="Permanent link">#</a></h4>
<p>In <strong><code>FreeFem++</code></strong> you can call MPI global communication functions.</p>
<p><code class="codehilite">broadcast(processor(int rk, mpiComm cc), Data D)</code>: Process <code class="codehilite">rk</code> Broadcast <code class="codehilite">Data D</code> to all process inside <code class="codehilite">communicator cc</code>. Note that <code class="codehilite">Data D</code> can be: <code class="codehilite">int</code>, <code class="codehilite">real</code>, <code class="codehilite">complex</code>, <code class="codehilite">int[int]</code>, <code class="codehilite">real[int]</code>, <code class="codehilite">complex[int]</code>, <code class="codehilite">Mesh</code>, <code class="codehilite">Mesh3</code>, <code class="codehilite">Matrix</code>.</p>
<p><code class="codehilite">broadcast(processor(int rk), Data D)</code>: Process <code class="codehilite">rk</code> Broadcast <code class="codehilite">Data D</code> to all process inside <code>MPI_COMM_WORLD</code>. Note that <code class="codehilite">Data D</code> can be: <code class="codehilite">int</code>, <code class="codehilite">real</code>, <code class="codehilite">complex</code>, <code class="codehilite">int[int]</code>, <code class="codehilite">real[int]</code>, <code class="codehilite">complex[int]</code>, <code class="codehilite">Mesh</code>, <code class="codehilite">Mesh3</code>, <code class="codehilite">Matrix</code>.</p>
<p><code class="codehilite">mpiAlltoall(Data a, Data b)</code>: Sends <code class="codehilite">data a</code> from all to all processes. Receive buffer is <code class="codehilite">Data b</code>. This is done inside communicator <code>MPI_COMM_WORLD</code>.</p>
<p><code class="codehilite">mpiAlltoall(Data a, Data b, mpiComm cc)</code>: Sends <code class="codehilite">data a</code> from all to all processes. Receive buffer is <code class="codehilite">Data b</code>. This is done inside communicator <code>cc</code>.</p>
<p><code class="codehilite">mpiGather(Data a, Data b, processor(mpiComm, int rk)</code>: Gathers together values <code class="codehilite">Data a</code> from a group of processes. Process of rank <code class="codehilite">rk</code> get data on communicator <code class="codehilite">rk</code>. This function is like <code>MPI_Gather</code>.</p>
<p><code class="codehilite">mpiAllgather(Data a, Data b)</code>: Gathers <code class="codehilite">Data a</code> from all processes and distribute it to all in <code class="codehilite">Data b</code>. This is done inside communicator <code>MPI_COMM_WORLD</code>. This function is like <code>MPI_Allgather</code>.</p>
<p><code class="codehilite">mpiAllgather(Data a, Data b, mpiComm cc)</code>: Gathers <code class="codehilite">Data a</code> from all processes and distribute it to all in <code class="codehilite">Data b</code>. This is done inside <code class="codehilite">communicator cc</code>. This function is like <code>MPI_Allgather</code>.</p>
<p><code class="codehilite">mpiScatter(Data a,Data b,processor(int rk, mpiComm cc))</code>: Sends <code class="codehilite">Data a</code> from one process whith rank <code class="codehilite">rk</code> to all other processes in group represented by communicator <code class="codehilite">mpiComm cc</code>.</p>
<p><code class="codehilite">mpiReduce(Data a, Data b, processor(int rk, mpiComm cc), MPI_Op op)</code> Reduces values <code class="codehilite">Data a</code> on all processes to a single value <code class="codehilite">Data b</code> on process of rank <code class="codehilite">rk</code> and communicator <code class="codehilite">cc</code>.</p>
<p>Operation use in reduce is: <code class="codehilite">MPI_Op op</code> which can be: <code class="codehilite">mpiMAX</code>, <code class="codehilite">mpiMIN</code>, <code class="codehilite">mpiSUM</code>, <code class="codehilite">mpiPROD</code>, <code class="codehilite">mpiLAND</code>, <code class="codehilite">mpiLOR</code>, <code class="codehilite">mpiLXOR</code>, <code class="codehilite">mpiBAND</code>, <code class="codehilite">mpiBXOR</code>, <code class="codehilite">mpiMAXLOC</code>, <code class="codehilite">mpiMINLOC</code>.</p>
<p>Note that, for all global operations, only <code class="codehilite">int[int]</code> and <code class="codehilite">real[int]</code> are data type take in account in <strong><code>FreeFem++</code></strong>.</p>
<h3 id="hpddm-solvers">HPDDM solvers<a class="headerlink" href="#hpddm-solvers" title="Permanent link">#</a></h3>
<p>Real valued problems (diffusion, heat, elasticity and Stokes) and complex valued problems (Maxwell and Helmholtz) are given in both 2D and 3D. We detail here the 3D elasticity problem and the 3D time-dependent heat problem.</p>
<div class="admonition example">
<p class="admonition-title">Elasticity 3D</p>
<p>A three dimensional elasticity problem is defined. The solver is a domain decomposition method. Domain decomposition methods are a natural framework for parallel computers. The scripts run on multicores computers (from 2 to tens of thousands of cores). Recall that like in any MPI code the number of MPI processes, <code class="codehilite">mpisize</code>, is given in the command line via the option <code class="codehilite">-np</code>. We focus on the script <code class="codehilite">Elasticity3D.edp</code> but the other scripts have the same structure. The command line to run the example on four processes with <code class="codehilite">ffglut</code> visualization is: <code class="codehilite">ff-mpirun -np 4 Elasticity3D.edp -glut ffglut</code></p>
<div class="codehilite"><pre><span></span>load &quot;hpddm&quot; //load HPDDM plugin
macro partitioner()metis//metis, scotch, or parmetis
macro dimension()3//2D or 3D
macro vectorialfe()P1//
include &quot;macro_ddm.idp&quot; //additional DDM functions

// Macro
macro def(i)[i, i#B, i#C] //vector field definition
macro init(i)[i, i, i] //vector field initialization

real Sqrt = sqrt(2.0);
macro epsilon(u) [dx(u), dy(u#B), dz(u#C),
    (dz(u#B) + dy(u#C)) / Sqrt,
    (dz(u) + dx(u#C)) / Sqrt,
    (dy(u) + dx(u#B)) / Sqrt] //
macro div(u) (dx(u) + dy(u#B) + dz(u#C)) //

// Parameters
real f = -9000.0;
real strain = 100.0;
real Young = 2.0e11; // steel
real poisson = 0.35;

func Pk = [vectorialfe, vectorialfe, vectorialfe];

string deflation = getARGV(&quot;-deflation&quot;, &quot;geneo&quot;); //coarse space construction
int overlap = getARGV(&quot;-overlap&quot;, 1); //geometric overlap between subdomains
int fakeInterface = getARGV(&quot;-interface&quot;, 10); //interface between subdomains
int s = getARGV(&quot;-split&quot;, 1); //refinement factor
int p = getARGV(&quot;-hpddm_master_p&quot;, 1);

mpiComm comm;
bool excluded = splitComm(mpiCommWorld, p, comm, topology = getARGV(&quot;-hpddm_master_topology&quot;, 0), exclude = (usedARGV(&quot;-hpddm_master_exclude&quot;) != -1));

// Display
if (verbosity &gt; 0 &amp;&amp; mpirank == 0){
    cout &lt;&lt; &quot; --- &quot; &lt;&lt; mpirank &lt;&lt; &quot;/&quot; &lt;&lt; mpisize;
    cout &lt;&lt; &quot; - Elasticity3D.edp - input parameters: refinement factor = &quot; &lt;&lt; s &lt;&lt; &quot; - overlap = &quot; &lt;&lt; overlap &lt;&lt; endl;
}

// Mesh
int[int] LL = [2, 3, 2, 1, 2, 2];
meshN ThBorder, Th = cube(1, 1, 1, [x, y, z]);
fespace Wh(Th, Pk); //local finite element space

int[int] arrayIntersection; //ranks of neighboring subdomains
int[int][int] restrictionIntersection(0); //local-to-neighbors renumbering
real[int] D; //partition of unity
{
    meshN ThGlobal = cube(10*getARGV(&quot;-global&quot;, 5), getARGV(&quot;-global&quot;, 5), getARGV(&quot;-global&quot;, 5), [10*x, y, z], label=LL); //global mesh
    build(Th, ThBorder, ThGlobal, fakeInterface, s, overlap, D, arrayIntersection, restrictionIntersection, Wh, Pk, comm, excluded, 3)
}

// Problem
real tmp = 1.0 + poisson;
real mu = Young / (2.0 * tmp);
real lambda = Young * poisson / (tmp * (1.0 - 2.0 * poisson));
real[int] rhs; //local right-hand side
matrix&lt;real&gt; Mat; //local operator
{ //local weak form
    meshN ThAugmented = Th + ThBorder;
    varf vPb (def(u), def(v))
        = intN(ThAugmented)(
              lambda * div(u) * div(v)
            + 2.0 * mu * (epsilon(u)&#39; * epsilon(v))
        )
        + intN(ThAugmented)(
              f * vC
        )
        + on(1, u=0.0, uB=0.0, uC=0.0)
        ;

    fespace WhAugmented(ThAugmented, Pk);
    Mat = vPb(WhAugmented, WhAugmented, tgv=-1);
    real[int] rhsFull = vPb(0, WhAugmented, tgv=-1);
    matrix R = interpolate(Wh, WhAugmented);
    renumbering(Mat, R, rhsFull, rhs);
}
ThBorder = cube(1, 1, 1, [x, y, z]);

dschwarz A(Mat, arrayIntersection, restrictionIntersection, scaling = D);

set(A, sparams = &quot;-hpddm_schwarz_method ras -hpddm_schwarz_coarse_correction balanced -hpddm_variant right -hpddm_verbosity 1 -hpddm_geneo_nu 10&quot;);

matrix&lt;real&gt; Opt; //local operator with optimized boundary conditions
dpair ret;
{
    int solver = getOption(&quot;schwarz_method&quot;);
    if (solver == 1 || solver == 2 || solver == 4){ //optimized Schwarz methods
        fespace Ph(Th, P0);
        real kZero = getARGV(&quot;-kZero&quot;, 10.0);
        Ph transmission = 2 * kZero * mu * (2 * mu + lambda) / (lambda + 3 * mu);
        varf vOptimized (def(u), def(v))
            = intN(Th)(
                  lambda * div(u) * div(v)
                + 2.0 * mu * (epsilon(u)&#39; * epsilon(v))
            )
            + intN1(Th, fakeInterface)(
                  transmission * (def(u)&#39; * def(v))
            )
            + on(1, u=0.0, uB=0.0, uC=0.0)
            ;
        Opt = vOptimized(Wh, Wh, tgv=-1);
    }
    if (mpisize &gt; 1 &amp;&amp; isSetOption(&quot;schwarz_coarse_correction&quot;)){ //two-level Schwarz methods
        if(excluded)
            attachCoarseOperator(mpiCommWorld, A);
        else {
            varf vPbNoPen (def(u), def(v))
                = intN(Th)(
                      lambda * div(u) * div(v)
                    + 2.0 * mu * (epsilon(u)&#39; * epsilon(v))
                )
                + on(1, u=0.0, uB=0.0, uC=0.0)
                ;
            matrix&lt;real&gt; noPen = vPbNoPen(Wh, Wh, solver=CG);
            if(deflation == &quot;geneo&quot;) //standard GenEO, no need for RHS -&gt; deduced from LHS (Neumann matrix)
                attachCoarseOperator(mpiCommWorld, A, A=noPen, ret=ret);
            else if(deflation == &quot;dtn&quot;){
                varf vMass (def(u), def(v)) = intN1(Th, fakeInterface)(u * v);
                matrix&lt;real&gt; massMatrix = vMass(Wh, Wh, solver=CG);
                attachCoarseOperator(mpiCommWorld, A, A=noPen, B=massMatrix, pattern=Opt, ret=ret);
            }
            else if(deflation == &quot;geneo-2&quot;) //GenEO-2 for optimized Schwarz methods, need for RHS (LHS is still Neumann matrix)
                attachCoarseOperator(mpiCommWorld, A, A=noPen, B=Opt, pattern=Opt, ret=ret);
        }
    }
}


// Solve
Wh&lt;real&gt; def(u); //local solution

if(Opt.n &gt; 0) //optimized Schwarz methods
    DDM(A, u[], rhs, excluded=excluded, ret=ret, O=Opt);
else
    u[] = A^-1 * rhs;

// Error
real[int] err(u[].n);
err = A * u[]; //global matrix-vector product
err -= rhs;

// Plot
plotMPI(Th, u[], &quot;Global solution&quot;, Pk, def, real, 3, 1)
plotMPI(Th, err, &quot;Global residual&quot;, Pk, def, real, 3, 1)
real alpha = 2000.0;
meshN ThMoved = movemesh3(Th, transfo = [x + alpha*u, y + alpha*uB, z + alpha*uC]);
u[] = mpirank;
plotMPI(ThMoved, u[], &quot;Global moved solution&quot;, Pk, def, real, 3, 1)
</pre></div>

<p>The macro <code class="codehilite">build</code> is of particular interest since it handles the data distribution among the <code class="codehilite">mpisize</code> MPI processes with the following steps:</p>
<ul>
<li>
<p>The initial mesh <code class="codehilite">ThGlobal</code> is partitioned by process 0 into <code class="codehilite">mpisize</code> submeshes</p>
</li>
<li>
<p>The partition is broadcasted to every process <span><span class="MathJax_Preview">i</span><script type="math/tex">i</script></span> for 0 &lt; <span><span class="MathJax_Preview">i</span><script type="math/tex">i</script></span> &lt; <code class="codehilite">mpisize</code>. From then on, all tasks are parallel.</p>
</li>
<li>
<p>Each process creates the local submesh <code class="codehilite">Th</code> (if the refinement factor <code class="codehilite">s</code> defined via the option <code class="codehilite">-split</code> is larger than 1, each local edge is splitted into <span><span class="MathJax_Preview">s</span><script type="math/tex">s</script></span> subedges, resulting in each element being split into <span><span class="MathJax_Preview">s^2</span><script type="math/tex">s^2</script></span> element in 2D and <span><span class="MathJax_Preview">s^3</span><script type="math/tex">s^3</script></span> elements in 3D) so that the collection of these submeshes is an overlapping domain decomposition of a refined mesh. The number of extra layers added to the initial partition is monitored by the option <code class="codehilite">-overlap</code>.</p>
</li>
<li>
<p>Connectivity structures are created</p>
<ul>
<li><code class="codehilite">D</code> is the diagonal of the local partition of unity (see <a href="#distributed-vectors-in-hpddm for more details">Distributed vectors in HPDDM</a></li>
<li><code class="codehilite">arrayIntersection</code> is the list of neighbors of the current subdomain</li>
<li>For <code class="codehilite">j</code> in <code class="codehilite">arrayIntersection</code>, <code class="codehilite">restrictionIntersection[j]</code> is the list of the degrees of freedom that belong to the intersection of the current subdomain with its neighbor <code class="codehilite">j</code>.</li>
</ul>
</li>
</ul>
<p>Then, the variational formulation <code class="codehilite">vPb</code> of a three dimensional elasticity problem is used to assemble a local matrix <code class="codehilite">Mat</code>. This matrix along with <code class="codehilite">D</code>, <code class="codehilite">arrayIntersection</code> and <code class="codehilite">restrictionIntersection</code> are arguments for the constructor of the distributed matrix <code class="codehilite">A</code>. This is enough to solve the problem with a one-level additive Schwarz method which can be either ASM or RAS.</p>
<p>For some problems it is interesting to use optimized interface conditions. When there are many subdomains, it is usually profitable to add a second level to the solver. Options are set in the sequel of the script:</p>
<div class="codehilite"><pre><span></span>set(A, sparams=&quot;-hpddm_schwarz_method ras -hpddm_schwarz_coarse_correction balanced -hpddm_variant right -hpddm_verbosity 1 -hpddm_geneo_nu 10&quot;);
</pre></div>

<p>In the above line, the first option selects the one-level preconditioner <code class="codehilite">ras</code> (possible choices are <code class="codehilite">ras</code>, <code class="codehilite">oras</code>, <code class="codehilite">soras</code>, <code class="codehilite">asm</code>, <code class="codehilite">osm</code> or <code class="codehilite">none</code>), the second option selects the correction formula for the second level here <code class="codehilite">balanced</code> (possible options are <code class="codehilite">deflated</code>, <code class="codehilite">additive</code> or <code class="codehilite">balanced</code>), the third option selects right preconditioning, the fourth one is verbosity level of HPDDM (different from the one of <strong><code>FreeFem++</code></strong>), the fifth one prints all possible options of HPPDM and the last one specifies the number of coarse degrees of freedom per subdomain of the GENEO coarse space. All other options of <a href="cheatsheet of the HPDDM">HPDDM library</a> can be selected via the <strong><code>FreeFem++</code></strong> function <code class="codehilite">set</code>.</p>
<p>In the last part of the script, the global linear system is solved by the domain decomposition method defined above.</p>
<div class="codehilite"><pre><span></span>// Solve
Wh&lt;real&gt; def(u); //local solution

if(Opt.n &gt; 0) //optimized Schwarz methods
    DDM(A, u[], rhs, excluded=excluded, ret=ret, O=Opt);
else
    u[] = A^-1 * rhs;
</pre></div>

</div>
<h3 id="time-dependent-problem">Time dependent problem<a class="headerlink" href="#time-dependent-problem" title="Permanent link">#</a></h3>
<div class="admonition example">
<p class="admonition-title">Heat 3D</p>
<p>A three dimensional heat problem</p>
<div>
<div class="MathJax_Preview">
\frac{\partial u}{\partial t} - \Delta u = 1,\ \ \ u(0,\cdot) := 0 \text{ in }\Omega\,.
</div>
<script type="math/tex; mode=display">
\frac{\partial u}{\partial t} - \Delta u = 1,\ \ \ u(0,\cdot) := 0 \text{ in }\Omega\,.
</script>
</div>
<p>is discretized by an implicit Euler scheme. At each time step <span><span class="MathJax_Preview">n</span><script type="math/tex">n</script></span>, we shall seek <span><span class="MathJax_Preview">u^n(x,y,z)</span><script type="math/tex">u^n(x,y,z)</script></span> satisfying for all <span><span class="MathJax_Preview">w\in H^1(\Omega)</span><script type="math/tex">w\in H^1(\Omega)</script></span>:</p>
<div>
<div class="MathJax_Preview">
\int_\Omega \frac{u^n-u^{n-1}}{\delta t}\,w + \nabla u^n \nabla w = \int_\Omega w ,\ \ \ u^0 := 0 \text{ in }\Omega\,.
</div>
<script type="math/tex; mode=display">
\int_\Omega \frac{u^n-u^{n-1}}{\delta t}\,w + \nabla u^n \nabla w = \int_\Omega w ,\ \ \ u^0 := 0 \text{ in }\Omega\,.
</script>
</div>
<p>so that at each time step a linear system</p>
<div>
<div class="MathJax_Preview">
(M+dt*K) u^n[] = M*u^{n-1}[] + \delta t*F
</div>
<script type="math/tex; mode=display">
(M+dt*K) u^n[] = M*u^{n-1}[] + \delta t*F
</script>
</div>
<p>is solved by a domain decomposition method where <span><span class="MathJax_Preview">M</span><script type="math/tex">M</script></span> is the mass matrix and <span><span class="MathJax_Preview">K</span><script type="math/tex">K</script></span> is the rigidity matrix. In order to save computational efforts, the domain decomposition method preconditioner is built only once and then reused for all subsequent solves with matrix <span><span class="MathJax_Preview">A:=M+dt*K</span><script type="math/tex">A:=M+dt*K</script></span>. The distributed matrix vector product with matrix <span><span class="MathJax_Preview">M</span><script type="math/tex">M</script></span> is made through the call to the function <code class="codehilite">dmv</code> using the partition of unity associated to matrix <span><span class="MathJax_Preview">A</span><script type="math/tex">A</script></span>.</p>
<div class="codehilite"><pre><span></span>load &quot;hpddm&quot; //load HPDDM plugin
macro partitioner()metis//metis, scotch, or parmetis
macro dimension()3//2D or 3D
include &quot;macro_ddm.idp&quot; //additional DDM functions

// Macro
macro def(i)i //scalar field definition
macro init(i)i //scalar field initialization
macro grad(u) [dx(u), dy(u), dz(u)] //three-dimensional gradient

// Parameters
func Pk = P2; //finite element space

string deflation = getARGV(&quot;-deflation&quot;, &quot;geneo&quot;); //coarse space construction
int overlap = getARGV(&quot;-overlap&quot;, 1); //geometric overlap between subdomains
int fakeInterface = getARGV(&quot;-interface&quot;, 10); //interface between subdomains
int s = getARGV(&quot;-split&quot;, 1); //refinement factor
real dt = getARGV(&quot;-dt&quot;, 0.01); //time step
int iMax = getARGV(&quot;-iMax&quot;, 10); //number of iterations

mpiComm comm;
int p = getARGV(&quot;-hpddm_master_p&quot;, 1);
bool excluded = splitComm(mpiCommWorld, p, comm, topology = getARGV(&quot;-hpddm_master_topology&quot;, 0), exclude = (usedARGV(&quot;-hpddm_master_exclude&quot;) != -1));

// Display
if (verbosity &gt; 0 &amp;&amp; mpirank == 0){
    cout &lt;&lt; &quot; --- &quot; &lt;&lt; mpirank &lt;&lt; &quot;/&quot; &lt;&lt; mpisize;
    cout &lt;&lt; &quot; - Heat3D.edp - input parameters: refinement factor = &quot; &lt;&lt; s &lt;&lt; &quot; - overlap = &quot; &lt;&lt; overlap &lt;&lt; endl;
}

// Mesh
int[int] LL = [1, 2, 1, 1, 1, 1];
meshN ThBorder, Th = cube(1, 1, 1, [x, y, z]);
fespace Wh(Th, Pk); //local finite element space
int[int] arrayIntersection; //ranks of neighboring subdomains
int[int][int] restrictionIntersection(0); //local-to-neighbors renumbering
real[int] D; //partition of unity
{
    meshN ThGlobal = cube(getARGV(&quot;-global&quot;, 10), getARGV(&quot;-global&quot;, 10), getARGV(&quot;-global&quot;, 10), [x, y, z], label=LL); //global mesh
    build(Th, ThBorder, ThGlobal, fakeInterface, s, overlap, D, arrayIntersection, restrictionIntersection, Wh, Pk, comm, excluded)
}

// Problem
real[int] rhs; // local right-hand side
matrix&lt;real&gt; Mat; //local operator
matrix&lt;real&gt; M; //local mass matrix
{ //local weak form
    meshN ThAugmented = Th + ThBorder;
    varf vPb (u, v)
        = intN(ThAugmented)(
              u * v
            + dt * (grad(u)&#39; * grad(v))
        )
        + intN(ThAugmented)(
              dt * v
        )
        + on(1, u=0.0)
        ;
    fespace WhAugmented(ThAugmented, Pk);
    Mat = vPb(WhAugmented, WhAugmented, tgv=-1);
    real[int] rhsFull = vPb(0, WhAugmented, tgv=-1);
    matrix R = interpolate(Wh, WhAugmented);
    varf vPbM (u, v) = intN(ThAugmented)(u * v);
    M = vPbM(WhAugmented, WhAugmented);
    renumbering(M, R, rhsFull, rhs);
    renumbering(Mat, R, rhsFull, rhs);
}
ThBorder = cube(1, 1, 1, [x, y, z]);

dschwarz A(Mat, arrayIntersection, restrictionIntersection, scaling=D);

matrix&lt;real&gt; Opt; //local operator with optimized boundary conditions
dpair ret;
{
    int solver = getOption(&quot;schwarz_method&quot;);
    if (solver == 1 || solver == 2 || solver == 4){ //optimized Schwarz methods
        fespace Ph(Th, P0);
        real kZero = getARGV(&quot;-kZero&quot;, 10.0);
        Ph transmission = kZero;
        varf vOptimized (u, v)
            = intN(Th)(
                  u * v
                + dt * (grad(u)&#39; * grad(v))
            )
            + intN1(Th, fakeInterface)(
                  transmission * (u * v)
            )
            + on(1, u=0.0)
            ;
        Opt = vOptimized(Wh, Wh, tgv=-1);
    }
    if (mpisize &gt; 1 &amp;&amp; isSetOption(&quot;schwarz_coarse_correction&quot;)){ //two-level Schwarz methods
        if(excluded)
            attachCoarseOperator(mpiCommWorld, A);
        else {
            varf vPbNoPen (u, v)
                = intN(Th)(
                      u * v
                    + dt * (grad(u)&#39; * grad(v))
                )
                + on(1, u=0.0)
                ;
            matrix&lt;real&gt; noPen = vPbNoPen(Wh, Wh, solver=CG);
            if(deflation == &quot;geneo&quot;) //standard GenEO, no need for RHS -&gt; deduced from LHS (Neumann matrix)
                attachCoarseOperator(mpiCommWorld, A, A=noPen, ret = ret);
            else if(deflation == &quot;dtn&quot;) {
                varf vMass (def(u), def(v)) = intN1(Th, fakeInterface)(u * v);
                matrix&lt;real&gt; massMatrix = vMass(Wh, Wh, solver=CG);
                attachCoarseOperator(mpiCommWorld, A, A=noPen, B=massMatrix, pattern=Opt, ret=ret);
            }
            else if(deflation == &quot;geneo-2&quot;) //GenEO-2 for optimized Schwarz methods, need for RHS (LHS is still Neumann matrix)
                attachCoarseOperator(mpiCommWorld, A, A=noPen, B=Opt, pattern=Opt, ret=ret);
        }
    }
}

// Solve
set(A, sparams=&quot;-hpddm_reuse_preconditioner=1&quot;);
Wh&lt;real&gt; def(u) = init(0.0); //local solution
for (int i = 0; i &lt; iMax; ++i){
    real[int] newRhs(rhs.n);
    dmv(A, M, u[], newRhs); //newRhs = M * u[]
    newRhs += rhs;

    if (Opt.n &gt; 0) //optimized Schwarz methods
        DDM(A, u[], newRhs, excluded=excluded, ret=ret, O=Opt);
    else
        u[] = A^-1 * newRhs;

    plotMPI(Th, u[], &quot;Global solution&quot;, Pk, def, real, 3, 0)
}
</pre></div>

</div>
<h3 id="distributed-vectors-in-hpddm">Distributed vectors in HPDDM<a class="headerlink" href="#distributed-vectors-in-hpddm" title="Permanent link">#</a></h3>
<p>We give here some hints on the way vectors are distributed among <span><span class="MathJax_Preview">np</span><script type="math/tex">np</script></span> processes when using <strong><code>FreeFem++</code></strong> interfaced with HPDDM. The set of degrees of freedom <span><span class="MathJax_Preview">{\mathcal N}</span><script type="math/tex">{\mathcal N}</script></span> is decomposed into <span><span class="MathJax_Preview">np</span><script type="math/tex">np</script></span> overlapping sets <span><span class="MathJax_Preview">({\mathcal N}_i)_{1\le i\le np}</span><script type="math/tex">({\mathcal N}_i)_{1\le i\le np}</script></span>.</p>
<!--- __ --->

<p>A MPI-process is in charge of each subset. Let <span><span class="MathJax_Preview">n:=\#{\mathcal N}</span><script type="math/tex">n:=\#{\mathcal N}</script></span> be the number of degrees of freedom of the global finite element space. Let <span><span class="MathJax_Preview">R_i</span><script type="math/tex">R_i</script></span> denote the restriction operator from <span><span class="MathJax_Preview">\R^n</span><script type="math/tex">\R^n</script></span> onto <span><span class="MathJax_Preview">\R^{\#{\mathcal N}_i}</span><script type="math/tex">\R^{\#{\mathcal N}_i}</script></span>. We have also defined local diagonal matrices <span><span class="MathJax_Preview">D_i\in \R^{\#{\mathcal N}_i}\times \R^{\#{\mathcal N}_i}</span><script type="math/tex">D_i\in \R^{\#{\mathcal N}_i}\times \R^{\#{\mathcal N}_i}</script></span> so that we have a partition of unity at the algebraic level:</p>
<div>
<div class="MathJax_Preview">\begin{equation}
    \label{eq:hpddm:14}
  {\mathbf U} = \sum_{i=1}^{np} R_i^T\,D_i\,R_i\,{\mathbf U}\ \ \ \ \forall\ {\mathbf U}\in\R^n\,.
\end{equation}</div>
<script type="math/tex; mode=display">\begin{equation}
    \label{eq:hpddm:14}
  {\mathbf U} = \sum_{i=1}^{np} R_i^T\,D_i\,R_i\,{\mathbf U}\ \ \ \ \forall\ {\mathbf U}\in\R^n\,.
\end{equation}</script>
</div>
<p>A global vector <span><span class="MathJax_Preview">{\mathbf U}\in\R^n</span><script type="math/tex">{\mathbf U}\in\R^n</script></span> is actually not stored. Rather, it is stored in a distributed way. Each process <span><span class="MathJax_Preview">i</span><script type="math/tex">i</script></span>, <span><span class="MathJax_Preview">1\le i\le N</span><script type="math/tex">1\le i\le N</script></span>, stores the local vector <span><span class="MathJax_Preview">{\mathbf U}_i:=R_i {\mathbf U}\in \R^{\#{\mathcal N}_i}</span><script type="math/tex">{\mathbf U}_i:=R_i {\mathbf U}\in \R^{\#{\mathcal N}_i}</script></span>.</p>
<p>It is important to ensure that the result of all linear algebra operators applied to this representation are coherent.</p>
<p>As an example, consider the scalar product of two distributed vectors <span><span class="MathJax_Preview">{\mathbf U}, {\mathbf V} \in \mathbb{R}^{n}</span><script type="math/tex">{\mathbf U}, {\mathbf V} \in \mathbb{R}^{n}</script></span>. Using the partition of unity \eqref{eq:hpddm:14}, we have:</p>
<div>
<div class="MathJax_Preview">\begin{align*}({\mathbf U}, {\mathbf V}) = \left({\mathbf U}, \sum_{i=1}^{np} R_i^T D_i R_i {\mathbf V}\right) &amp;= \sum_{i=1}^{np} (R_i {\mathbf U}, D_i R_i {\mathbf V})\\
&amp;=\sum_{i=1}^{np} \left({\mathbf U}_i, D_i {\mathbf V}_i\right)\,.
\end{align*}</div>
<script type="math/tex; mode=display">\begin{align*}({\mathbf U}, {\mathbf V}) = \left({\mathbf U}, \sum_{i=1}^{np} R_i^T D_i R_i {\mathbf V}\right) &= \sum_{i=1}^{np} (R_i {\mathbf U}, D_i R_i {\mathbf V})\\
&=\sum_{i=1}^{np} \left({\mathbf U}_i, D_i {\mathbf V}_i\right)\,.
\end{align*}</script>
</div>
<p>Thus, the formula for the scalar product is:</p>
<div>
<div class="MathJax_Preview">\begin{equation*}
({\mathbf U}, {\mathbf V}) = \sum_{i = 1}^{np} (R_i {\mathbf U}, D_i R_i {\mathbf V})\,.
\end{equation*}</div>
<script type="math/tex; mode=display">\begin{equation*}
({\mathbf U}, {\mathbf V}) = \sum_{i = 1}^{np} (R_i {\mathbf U}, D_i R_i {\mathbf V})\,.
\end{equation*}</script>
</div>
<p>Local scalar products are performed concurrently. Thus, the implementation is parallel except for the sum which corresponds to a <code class="codehilite">MPI_Reduce</code> call across the <span><span class="MathJax_Preview">np</span><script type="math/tex">np</script></span> MPI processes. Note also that the implementation relies on the knowledge of a partition of unity so that the FreeFem++ syntax is <code class="codehilite">dscalprod(D, u, v)</code>.</p>
<p>A <code class="codehilite">axpy</code> procedure <span><span class="MathJax_Preview">y \leftarrow \alpha\,x+y</span><script type="math/tex">y \leftarrow \alpha\,x+y</script></span> for <span><span class="MathJax_Preview">x,y\in \mathbb{R}^{n}</span><script type="math/tex">x,y\in \mathbb{R}^{n}</script></span> and <span><span class="MathJax_Preview">\alpha\in\R</span><script type="math/tex">\alpha\in\R</script></span> is easily implemented concurrently for distributed vectors in the form:</p>
<div>
<div class="MathJax_Preview">
y_i \leftarrow \alpha\,x_i+y_i\,, \forall\ 1\le i \le np\,.
</div>
<script type="math/tex; mode=display">
y_i \leftarrow \alpha\,x_i+y_i\,, \forall\ 1\le i \le np\,.
</script>
</div>
<p>The matrix vector product is more involved and details are given in the SIAM book  <a href="https://www.ljll.math.upmc.fr/nataf/OT144DoleanJolivetNataf_full.pdf">An Introduction to Domain Decomposition Methods: algorithms, theory and parallel implementation</a> and even more details are given in <a href="http://jolivet.perso.enseeiht.fr/thesis.pdf">P. Jolivet's PhD manuscrit</a>.</p>
<h2 id="references">References<a class="headerlink" href="#references" title="Permanent link">#</a></h2>
<p><a name="KARYPIS1995">[KARYPIS1995]</a> KARYPIS, George et KUMAR, Vipin. METIS--unstructured graph partitioning and sparse matrix ordering system, version 2.0. 1995.</p>
<p><a name="CAI1989">[CAI1989]</a> CAI, Xiao-Chuan. Some domain decomposition algorithms for nonselfadjoint elliptic and parabolic partial differential equations. 1989.</p>
<p><a name="SAAD2003">[SAAD2003]</a> SAAD, Yousef. Iterative methods for sparse linear systems. siam, 2003.</p>
<p><a name="SMITH1996">[SMITH1996]</a> SMITH, B. P. Bj rstad and W. Gropp, Domain Decomposition. 1996.</p>
                
                  
                
              
              
                


              
            </article>
          </div>
        </div>
      </main>
      
        
<footer class="md-footer">
  
    <div class="md-footer-nav">
      <nav class="md-footer-nav__inner md-grid">
        
          <a href="../AlgorithmsOptimization/" title="Algorithms & Optimization" class="md-flex md-footer-nav__link md-footer-nav__link--prev" rel="prev">
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-back md-footer-nav__button"></i>
            </div>
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  Previous
                </span>
                Algorithms & Optimization
              </span>
            </div>
          </a>
        
        
          <a href="../Plugins/" title="Plugins" class="md-flex md-footer-nav__link md-footer-nav__link--next" rel="next">
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  Next
                </span>
                Plugins
              </span>
            </div>
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-forward md-footer-nav__button"></i>
            </div>
          </a>
        
      </nav>
    </div>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
        powered by
        <a href="https://www.mkdocs.org">MkDocs</a>
        and
        <a href="https://squidfunk.github.io/mkdocs-material/">
          Material for MkDocs</a>
      </div>
      
        
  <div class="md-footer-social">
    <link rel="stylesheet" href="../../assets/fonts/font-awesome.css">
    
      <a href="https://github.com/FreeFem/FreeFem-doc" class="md-footer-social__link fa fa-github"></a>
    
  </div>

      
    </div>
  </div>
</footer>
      
    </div>
    
      <script src="../../assets/javascripts/application.583bbe55.js"></script>
      
      <script>app.initialize({version:"1.0.4",url:{base:"../.."}})</script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML"></script>
      
        <script src="../../mathjax-config.js"></script>
      
    
    
      
    
  </body>
</html>